#!/usr/bin/env bash
#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# For any questions about this software or licensing,
# please email opensource@seagate.com or cortx-questions@seagate.com.
#

set -eu -o pipefail
export PS4='+ [${BASH_SOURCE[0]##*/}:${LINENO}${FUNCNAME[0]:+:${FUNCNAME[0]}}] '
set -x

. /data/hare/ci/_env  # HARE_CI_S3_ACCESS_KEY, HARE_CI_S3_SECRET_KEY
. /data/hare/ci/functions.sh  # _time

export PATH="/opt/seagate/cortx/hare/bin:/opt/seagate/cortx/hare/libexec:$PATH"

generate_s3cfg() {
    . update-consul-conf --dry-run  # import CONFD_IDs, IOS_IDs, id2fid()
    local id=${S3_IDs[0]}  # In absence of haproxy using 1st S3 server instance
    local ip_addr=$(get_service_ip_addr $(get_service_ep $id))
    local port=$(awk -F= '$1 == "MERO_S3SERVER_PORT" {print $2}' \
                     /etc/sysconfig/s3server-$(id2fid $id))
    cat <<EOF
[default]
access_key = $HARE_CI_S3_ACCESS_KEY
bucket_location = US
check_ssl_certificate = False
check_ssl_hostname = True
cloudfront_host = s3.seagate.com
default_mime_type = binary/octet-stream
delay_updates = False
delete_after = False
delete_after_fetch = False
delete_removed = False
dry_run = False
enable_multipart = True
encrypt = False
follow_symlinks = False
force = False
get_continue = False
gpg_command = /usr/bin/gpg
gpg_decrypt = %(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s
gpg_encrypt = %(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s
gpg_passphrase = seagate
guess_mime_type = True
host_base = s3.seagate.com
host_bucket = %(bucket)s.s3.seagate.com
human_readable_sizes = False
invalidate_default_index_on_cf = False
invalidate_default_index_root_on_cf = True
invalidate_on_cf = False
limit = -1
limitrate = 0
list_md5 = False
log_target_prefix =
long_listing = False
max_delete = -1
multipart_chunk_size_mb = 15
multipart_max_chunks = 10000
preserve_attrs = True
progress_meter = True
proxy_host = $ip_addr
proxy_port = $port
put_continue = False
recursive = False
recv_chunk = 65536
reduced_redundancy = False
requester_pays = False
restore_days = 1
restore_priority = Standard
secret_key = $HARE_CI_S3_SECRET_KEY
send_chunk = 65536
server_side_encryption = False
signature_v2 = False
signurl_use_https = False
simpledb_host = s3.seagate.com
skip_existing = False
socket_timeout = 300
stats = False
stop_on_error = False
storage_class =
throttle_max = 100
urlencoding_mode = normal
use_http_expect = False
use_https = False
use_mime_magic = True
verbosity = WARNING
website_endpoint = http://%(bucket)s.s3-website-%(location)s.seagate.com/
website_index = index.html
EOF
}

do_s3_ops() {
    local s3cfg=ci/_s3cfg
    local ifile=/tmp/128M
    local ofile=/tmp/128M.out
    local bucket=s3://seagate

    generate_s3cfg >$s3cfg

    dd if=/dev/urandom of=$ifile bs=1M count=128

    s3cmd --config $s3cfg mb $bucket
    s3cmd --config $s3cfg put $ifile $bucket

    s3cmd --config $s3cfg get $bucket/${ifile##*/} $ofile
    s3cmd --config $s3cfg del $bucket/${ifile##*/}
    s3cmd --config $s3cfg rb $bucket

    cmp $ifile $ofile
    rm -f $ifile $ofile
}

wait_for_s3server_to_start() {
    local fid=$1
    local state=$2
    local count=10

    set +x
    until [[ $(utils/get-process-state $fid) == $state ]]; do
        sleep 3
        echo -n '.'
        ((--count > 0)) || die 'Process not started'
    done
    set -x
}

cd /data/hare/

cdf=ci/m0vg/_test-boot1-s3.yaml
sed 's/s3: 0/s3: 2/' cfgen/examples/singlenode.yaml >$cdf

s3auth-disable
_time hctl bootstrap --mkfs $cdf
hctl status

. update-consul-conf --dry-run  # import CONFD_IDs, IOS_IDs, id2fid

# In the absence of HAProxy (load balancer for S3 server instances),
# let CI job use port number of the first S3 server instance as
# HAProxy port.
#
# XXX FIXME Add haproxy for CI.
wait_for_s3server_to_start $(id2fid ${S3_IDs[0]}) M0_CONF_HA_PROCESS_STARTED

time do_s3_ops
_time hctl shutdown
