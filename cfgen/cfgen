#!/usr/bin/env python3

import abc
import argparse
import ast
from enum import Enum, auto
from functools import lru_cache
import itertools
import json
import os
from pprint import pprint
import random
import re
import socket
import subprocess
import sys
from typing import Any, Callable, Dict, Iterator, List, NamedTuple, Optional, \
    Set, Tuple
import yaml


__version__ = '0.12.1'


def parse_opts(argv):
    p = argparse.ArgumentParser(description='Generate configuration files'
                                ' required to start Motr cluster.',
                                usage='%(prog)s [OPTION]... CDF')
    p.add_argument('CDF', type=argparse.FileType(),
                   help='cluster description file;'
                   " use '--help-schema' option for format description")
    p.add_argument('--help-schema', nargs=0,
                   help='show the schema of cluster description file (CDF)',
                   action=ShowSchema)
    default_dhall_dir = '/opt/seagate/cortx/hare/share/cfgen/dhall'
    p.add_argument('-D', '--dhall', metavar='dir',
                   help='directory with auxiliary Dhall expressions'
                   f' (defaults to {default_dhall_dir!r})',
                   dest='dhall_dir', default=default_dhall_dir)
    p.add_argument('-o', metavar='output-dir',
                   help="output directory (defaults to '.')",
                   dest='output_dir', default='.')
    p.add_argument('--mock', help='Generate pseudo-random "facts". The hosts'
                   ' specified in the cluster description file will not be'
                   " visited and don't even have to exist.",
                   action='store_true')
    p.add_argument('--debug',
                   help='print the enriched cluster description and exit',
                   action='store_true')
    p.add_argument('-V', '--version', action='version',
                   version='%(prog)s ' + __version__)
    opts = p.parse_args(argv)

    # Sanity check.
    dirs = [('--dhall', opts.dhall_dir, os.R_OK)]
    if not opts.debug:
        dirs.append(('-o', opts.output_dir, os.W_OK))
    for opt, d, perm in dirs:
        if not (d and os.path.isdir(d) and os.access(d, os.X_OK | perm)):
            what = 'read' if perm == os.R_OK else 'writ'
            die(f'{opt!r} argument must be a path to {what}able directory')

    opts.dhall_dir = os.path.abspath(opts.dhall_dir)
    return opts


class ShowSchema(argparse.Action):
    def __call__(self, *args):
        # XXX-TODO: Rename `nodes.hostname` to `nodes.name`.
        print("""\
# Cluster Description is a YAML file with the following schema:
---  # start of the document (optional)
nodes:
  - hostname: <str>    # [user@]hostname; e.g., localhost, pod-c1
    data_iface: <str>  # name of network interface; e.g., eth1, eth1:c1
    data_iface_type: tcp|o2ib  # type of network interface;
                               # optional, defaults to "tcp"
    m0_servers:
      - runs_confd: <bool>  # optional, defaults to false
        io_disks:
          meta_data: <str>  # device path for meta-data;
                            # optional, Motr will use "/var/motr/m0d-<FID>/"
                            # by default
          data: [ <str> ]   # e.g. [ "/dev/loop0", "/dev/loop1", "/dev/loop2" ]
                            # Empty list means no IO service.
    m0_clients:
        s3: <int>     # number of S3 servers to start
        other: <int>  # max quantity of other Motr clients this host may have
pools:
  - name: <str>
    type: sns|dix|md   # optional, defaults to "sns";
                       # "sns" - data pool, "dix" - KV, "md" - meta-data pool.
    data_units: <int>
    parity_units: <int>
    allowed_failures:  # optional section; no failures will be allowed
                       # if this section is missing or all of its elements
                       # are zeroes
      site: <int>
      rack: <int>
      encl: <int>
      ctrl: <int>
      disk: <int>
...  # end of the document (optional)""")
        sys.exit()


def main(argv=None):
    opts = parse_opts(argv)

    check_dhall_versions()

    cdf = opts.CDF.read()
    opts.CDF.close()
    validate_cdf_schema(cdf,
                        cdf_path=opts.CDF.name,
                        schema_path=os.path.join(
                            opts.dhall_dir, 'types', 'ClusterDesc.dhall'))
    cluster_desc = yaml.safe_load(cdf)

    enrich_cluster_desc(cluster_desc, opts.mock)
    validate_cluster_desc(cluster_desc)

    if opts.debug:
        pprint(cluster_desc)
        return

    cluster = build_cluster(cluster_desc)

    outs: List[Tuple[str, Callable[..., str], List[Any]]] = [
        ('consul-agents.json', generate_consul_agents, cluster),
        ('consul-kv.json', generate_consul_kv, cluster, opts.dhall_dir),
        ('confd.dhall', generate_confd, cluster.m0conf, opts.dhall_dir)]
    for path, generate, *args in outs:
        with open(os.path.join(opts.output_dir, path), 'w') as f:
            f.write(generate(*args))


def die(msg: str, status: int = 1):
    assert msg
    assert status != 0
    print(os.path.basename(sys.argv[0]) + ': ' + msg, file=sys.stderr)
    sys.exit(status)


def all_unique(xs) -> bool:
    """Returns True iff all entries of the sequence are unique.
    """
    if hasattr(xs, '__iter__') and hasattr(xs, '__next__'):
        # `xs` is a generator.  We should not consume it twice.
        xs = list(xs)
    return len(xs) == len(set(xs))


Version = NamedTuple('Version',
                     [('major', int), ('minor', int), ('patch', int)])


def version(s: str) -> Version:
    # We could have used `packaging.version` module, but it's not
    # worth to add an external dependency.
    return Version(*map(int, s.split('.', 2)))


def check_dhall_versions() -> None:
    # See https://github.com/dhall-lang/dhall-haskell/releases
    versions = {'dhall': version('1.26.1'), 'yaml-to-dhall': version('1.4.1')}

    for exe, ver in versions.items():
        try:
            proc = subprocess.Popen([exe, '--version'], stdout=subprocess.PIPE)
        except FileNotFoundError as err:
            print(str(err), file=sys.stderr)
            die(f'{exe} >= {ver} required, none found')

        out = proc.communicate(timeout=15)[0]
        if version(out.strip().decode()) < ver:
            die(f'{exe} >= {ver} required')


def validate_cdf_schema(cdf: str, cdf_path: str, schema_path: str) -> None:
    assert os.path.isabs(schema_path)
    proc = subprocess.Popen(['yaml-to-dhall', schema_path],
                            stdin=subprocess.PIPE,
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE)
    err = proc.communicate(input=cdf.encode(), timeout=15)[1]
    if proc.returncode != 0:
        die(f'**ERROR** {cdf_path}: Invalid cluster description\n' +
            err.decode().strip() +
            f"\n\nTo learn more, run '{sys.argv[0]} --help-schema'.",
            proc.returncode)


def ipaddr_key(iface: str) -> str:
    return 'ipaddress_' + iface


def enrich_cluster_desc(desc: Dict[str, Any], mock_p: bool) -> None:
    for node in desc['nodes']:
        # The fact names used here correspond to version 2.4.1 of `facter`.
        # See https://puppet.com/docs/puppet/6.6/core_facts.html#legacy-facts
        node['facts'] = get_facts(node['hostname'], mock_p,
                                  'processorcount', 'memorysize_mb',
                                  ipaddr_key(node['data_iface']))
        node['facts']['_memsize_MB'] = int(float(
            node['facts']['memorysize_mb']))

        for m0d in node['m0_servers']:
            m0d.setdefault('runs_confd', False)
            m0d['_io_disks'] = get_disks(node['hostname'], mock_p,
                                         m0d['io_disks']['data'])


def validate_cluster_desc(desc: Dict[str, Any]) -> None:
    Node = NamedTuple('Node', [('name', str), ('ipaddr', str)])
    nodes = [Node(name=x['hostname'],
                  ipaddr=x['facts'][ipaddr_key(x['data_iface'])])
             for x in desc['nodes']]
    assert all_unique(x.name for x in nodes), \
        "Node names ('hostname' values in the CDF) are not unique:\n" + \
        '\n'.join('  ' + x.name for x in nodes)
    assert all_unique(x.ipaddr for x in nodes), \
        'IP addresses are not unique:\n' + \
        '\n'.join('  ' + x.ipaddr for x in nodes)

    total_nr_confds = total_nr_disks = 0

    for node in desc['nodes']:
        name = node['hostname']
        assert name
        iface = node['data_iface']
        assert iface
        assert node['facts'][ipaddr_key(iface)], \
            f"""{name}: {iface!r} interface has no IP address
Make sure the value of data_iface in the CDF is correct."""

        nr_confds = sum(1 for m0d in node['m0_servers'] if m0d['runs_confd'])
        assert nr_confds < 2, f'{name}: Too many confd services'
        total_nr_confds += nr_confds

        for m0d in node['m0_servers']:
            d = m0d['io_disks']
            assert m0d['runs_confd'] or d['data'], \
                f"{name}: Either 'runs_confd' or 'io_disks.data' must be set"
            assert '' not in d['data'], \
                f"{name}: Empty strings in 'io_disks.data' are not allowed"
            if 'meta_data' in d:
                assert d['meta_data'], \
                    f"{name}: 'io_disks.meta_data' must not be an empty string"
                assert d['meta_data'] not in d['data'], \
                    f"{name}: Meta-data disk must not belong io_disks.data"

        disks: List[Disk] = []
        for m0d in node['m0_servers']:
            disks.extend(m0d['_io_disks'])
        assert all_unique(disks), \
            f'{name}: Same disk used by several IO services'
        total_nr_disks += len(disks)

        assert (nr_confds + node['m0_clients']['s3']
                + node['m0_clients']['other'] > 0), \
            f'{name}: At least one Motr server or client is required'

    assert total_nr_confds > 0, 'At least one confd is required'
    assert total_nr_disks > 0, 'No disks found'

    pool_types = set()
    for pool in desc['pools']:
        name = pool['name']
        t = pool.get('type', 'sns')
        if t == 'sns':
            d = pool.get('allowed_failures')
            assert d is None or d['disk'] <= pool['parity_units'], """\
{}: Cannot allow that many disk failures ({})
The number of allowed disk failures must not exceed the number of spare disks\
 (parity_units={}).""".format(name, d['disk'], pool['parity_units'])
        else:
            data_units = pool['data_units']
            assert data_units == 1, f"""\
{name}: Wrong number of data_units ({data_units})
Pools of {t!r} type can only have 1 data unit."""
            assert t not in pool_types, \
                f'{name}: No more than one {t!r} pool can be defined'
        pool_types.add(t)
    assert 'sns' in pool_types, \
        "At least one pool of 'sns' type must be defined"


@lru_cache(maxsize=1)
def minion_id() -> Optional[str]:
    path = '/etc/salt/minion_id'
    if not os.path.isfile(path):
        return None
    with open(path) as f:
        return f.readline().strip()


def is_localhost(hostname: str) -> bool:
    assert hostname
    return hostname in ('localhost', '127.0.0.1', minion_id(),
                        socket.gethostname(), socket.getfqdn())


def run_command(hostname: str, *args: str) -> str:
    assert hostname
    return subprocess.check_output(args if is_localhost(hostname) else
                                   ['ssh', hostname, *args],
                                   timeout=15).decode()


def get_facts(hostname: str, mock_p: bool, *args: str) -> Dict[str, Any]:
    if mock_p:
        return fabricate_facts(hostname, *args)
    return json.loads(run_command(hostname, 'facter', '--json', *args))


def fabricate_facts(hostname: str, *args: str) -> Dict[str, Any]:
    rng = random.randrange
    fabricated = {
        'processorcount': rng(1, 21),
        'memorysize_mb': '{:.2f}'.format(random.uniform(512, 16000)),
        'ipaddress_eth1': '.'.join(str(n) for n in [
            random.choice([10, 172, 192]), rng(256), rng(256), rng(256)]),
    }
    return dict((k, fabricated[k]) for k in args)


Disk = NamedTuple('Disk', [('path', str), ('size', int), ('blksize', int)])
Disk.size.__doc__ = 'Total size, in bytes'
Disk.blksize.__doc__ = 'Block size for file system I/O'


# XXX see build_disk_info() in motr's `utils/m0genfacts`
def get_disks(hostname: str, mock_p: bool, paths: List[str]) -> List[Disk]:
    if not paths:
        return []
    if mock_p:
        return fabricate_disks(hostname, paths)

    code = f"""\
import io
import os

# os.path.getsize() and os.stat().st_size don't work well with loop devices,
# they always return 0.
def blockdev_size(path):
    with open(path, 'rb') as f:
        return f.seek(0, io.SEEK_END)

print([dict(path=path,
            size=blockdev_size(path),
            blksize=os.stat(path).st_blksize)
       for path in {paths!r}])
"""
    if not is_localhost(hostname):
        code = f'"{code}"'

    return [Disk(**kwargs) for kwargs in
            ast.literal_eval(run_command(hostname,
                                         'sudo', 'python3', '-c', code))]


def fabricate_disks(hostname: str, paths: List[str]) -> List[Disk]:
    assert paths
    return [Disk(path=x, size=0, blksize=4096) for x in paths]


ObjT = Enum('ObjT', 'root fdmi_flt_grp fdmi_filter'
            ' node process service sdev'  # software subtree
            ' site rack enclosure controller drive'  # hardware subtree
            ' pool pver pver_f objv'  # pools subtree
            ' profile')
ObjT.__doc__ = 'Motr conf object type'


def objT_to_dhall(t: ObjT) -> str:
    return 'types.ObjT.' + ''.join(s.capitalize() for s in t.name.split('_'))


Oid = NamedTuple('Oid', [('type', ObjT), ('fidk', int)])
Oid.__doc__ = 'Motr conf object identifier'
Oid.fidk.__doc__ = '.f_key part of the corresponding m0_fid'


def _infinite_counter(start: int = 0) -> Iterator[int]:
    k = start
    while True:
        yield k
        k += 1


fidk_gen = _infinite_counter()  # fid key generator


def new_oid(objt: ObjT) -> Oid:
    return Oid(objt, next(fidk_gen))


def oid_to_dhall(oid: Oid) -> str:
    return f'utils.zoid {objT_to_dhall(oid.type)} {oid.fidk}'


def oids_to_dhall(oids: List[Oid]) -> str:
    if not oids:
        return '[] : List types.Oid'
    return '[ {} ]'.format(', '.join(oid_to_dhall(x) for x in oids))


NetProtocol = Enum('NetProtocol', 'tcp o2ib')


class Endpoint:
    _tmids: Dict[Tuple[str, int], int] = {}

    def __init__(self, proto: NetProtocol, ipaddr: str, portal: int,
                 tmid: int = None):
        self.proto = proto
        assert ipaddr
        self.ipaddr = ipaddr
        assert portal > 0
        self.portal = portal
        if tmid is None:
            self.tmid = self._tmids.setdefault((ipaddr, portal), 1)
            self._tmids[(ipaddr, portal)] += 1
        else:
            assert tmid > 0
            self.tmid = tmid

    def __repr__(self):
        args = ', '.join([f'proto={self.proto!r}',
                          f'ipaddr={self.ipaddr!r}',
                          f'portal={self.portal}'
                          f'tmid={self.tmid}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self):
        return f'utils.endpoint types.Protocol.{self.proto.name}' \
            f' "{self.ipaddr}" {self.portal} {self.tmid}'

    def __str__(self):
        ip = self.ipaddr
        proto = self.proto.name
        return f'{ip}@{proto}:12345:{self.portal}:{self.tmid}'


class ToDhall(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def to_dhall(self, oid: Oid) -> str:
        pass


Downlink = NamedTuple('Downlink', [('name', str), ('children_t', ObjT)])
Downlink.__doc__ = 'parent -> children relation'


class ConfRoot(ToDhall):
    _objt = ObjT.root
    _downlinks = {Downlink('nodes', ObjT.node),
                  Downlink('sites', ObjT.site),
                  Downlink('pools', ObjT.pool),
                  Downlink('profiles', ObjT.profile)}  # XXX + fdmi_flt_grps

    def __init__(self, nodes: List[Oid], sites: List[Oid], pools: List[Oid],
                 profiles: List[Oid], mdpool: Oid = None,
                 imeta_pver: Oid = None):
        assert all(x.type is ObjT.node for x in nodes)
        assert all(x.type is ObjT.site for x in sites)
        assert all(x.type is ObjT.pool for x in pools)
        assert all(x.type is ObjT.profile for x in profiles)
        assert mdpool is None or mdpool.type is ObjT.pool
        assert imeta_pver is None or imeta_pver.type is ObjT.pver

        self.nodes = nodes
        self.sites = sites
        self.pools = pools
        self.profiles = profiles
        self.mdpool = mdpool
        self.imeta_pver = imeta_pver

    def __repr__(self):
        args = ', '.join([f'nodes={self.nodes}',
                          f'sites={self.sites}',
                          f'pools={self.pools}',
                          f'profiles={self.profiles}',
                          f'mdpool={self.mdpool}',
                          f'imeta_pver={self.imeta_pver}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.root
        assert self.imeta_pver is not None
        assert self.mdpool is not None
        args = ', '.join([
            f'id = {oid_to_dhall(oid)}',
            f'mdpool = {oid_to_dhall(self.mdpool)}',
            f'imeta_pver = Some ({oid_to_dhall(self.imeta_pver)})',
            f'nodes = {oids_to_dhall(self.nodes)}',
            f'sites = {oids_to_dhall(self.sites)}',
            f'pools = {oids_to_dhall(self.pools)}',
            f'profiles = {oids_to_dhall(self.profiles)}'
        ])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any]) -> Oid:
        root_id = new_oid(ObjT.root)
        m0conf[root_id] = cls(nodes=[], sites=[], pools=[], profiles=[])
        return root_id


class ConfNode(ToDhall):
    _objt = ObjT.node
    _downlinks = {Downlink('processes', ObjT.process)}

    def __init__(self, name: str, nr_cpu: int, memsize_MB: int,
                 processes: List[Oid]):
        self.name = name
        self.nr_cpu = nr_cpu
        self.memsize_MB = memsize_MB
        assert all(x.type is ObjT.process for x in processes)
        self.processes = processes

    def __repr__(self):
        args = ', '.join([f'nr_cpu={self.nr_cpu}',
                          f'memsize_MB={self.memsize_MB}',
                          f'processes={self.processes}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.node
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'nr_cpu = {self.nr_cpu}',
                          f'memsize_MB = {self.memsize_MB}',
                          f'processes = {oids_to_dhall(self.processes)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              node_desc: Dict[str, Any]) -> Oid:
        assert parent.type is ObjT.root
        node_id = new_oid(ObjT.node)
        facts = node_desc['facts']
        m0conf[node_id] = cls(name=node_desc['hostname'],
                              nr_cpu=facts['processorcount'],
                              memsize_MB=facts['_memsize_MB'],
                              processes=[])
        m0conf[parent].nodes.append(node_id)
        return node_id


ProcT = Enum('ProcT', 'hax m0_server m0_client_s3 m0_client_other')
ProcT.__doc__ = 'Type of process'


class ConfProcess(ToDhall):
    _objt = ObjT.process
    _downlinks = {Downlink('services', ObjT.service)}

    def __init__(self, nr_cpu: int, memsize_MB: int, endpoint: Endpoint,
                 services: List[Oid], meta_data: str = None):
        assert nr_cpu > 0
        self.nr_cpu = nr_cpu
        assert memsize_MB > 0
        self.memsize_MB = memsize_MB
        self.endpoint = endpoint
        self.meta_data = meta_data
        assert all(x.type is ObjT.service for x in services)
        self.services = services

    def __repr__(self):
        args = ', '.join([f'nr_cpu={self.nr_cpu}',
                          f'memsize_MB={self.memsize_MB}',
                          f'endpoint={self.endpoint}',
                          f'services={self.services}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.process
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'nr_cpu = {self.nr_cpu}',
                          f'memsize_MB = {self.memsize_MB}',
                          f'endpoint = {self.endpoint.to_dhall()}',
                          f'services = {oids_to_dhall(self.services)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls,
              m0conf: Dict[Oid, Any],
              parent: Oid,
              node_desc: Dict[str, Any],
              proc_t: ProcT,
              proc_desc: Dict[str, Any] = None,
              ctrl: Oid = None) -> Oid:
        assert parent.type is ObjT.node
        assert ctrl is None or ctrl.type is ObjT.controller

        facts = node_desc['facts']
        proto = node_desc.get('data_iface_type', 'tcp')
        ep = Endpoint(proto=NetProtocol[proto],
                      ipaddr=facts[ipaddr_key(node_desc['data_iface'])],
                      portal=proc_t.value)
        proc_id = new_oid(ObjT.process)

        meta_data = None
        if proc_desc is not None and proc_t is ProcT.m0_server:
            meta_data = proc_desc['io_disks'].get('meta_data')

        m0conf[proc_id] = cls(nr_cpu=facts['processorcount'],
                              memsize_MB=facts['_memsize_MB'],
                              endpoint=ep,
                              meta_data=meta_data,
                              services=[])
        m0conf[parent].processes.append(proc_id)

        for stype in service_types(proc_t, proc_desc):
            svc_id = ConfService.build(m0conf, proc_id, stype, ep)
            if stype is SvcT.M0_CST_IOS:  # XXX What about M0_CST_CAS?
                assert proc_desc is not None and ctrl is not None
                for disk in proc_desc['_io_disks']:
                    ConfDrive.build(m0conf, ctrl,
                                    ConfSdev.build(m0conf, svc_id, disk))
        return proc_id


# m0_conf_service_type
class SvcT(Enum):
    """Motr service type
    """
    M0_CST_MDS = 1
    M0_CST_IOS = auto()
    M0_CST_CONFD = auto()
    M0_CST_RMS = auto()
    M0_CST_STATS = auto()
    M0_CST_HA = auto()
    M0_CST_SSS = auto()
    M0_CST_SNS_REP = auto()
    M0_CST_SNS_REB = auto()
    M0_CST_ADDB2 = auto()
    M0_CST_CAS = auto()
    M0_CST_DIX_REP = auto()
    M0_CST_DIX_REB = auto()
    M0_CST_DS1 = auto()
    M0_CST_DS2 = auto()
    M0_CST_FIS = auto()
    M0_CST_FDMI = auto()
    M0_CST_BE = auto()
    M0_CST_M0T1FS = auto()
    M0_CST_CLIENT = auto()
    M0_CST_ISCS = auto()

    def to_dhall(self) -> str:
        return f'types.{self}'


def service_types(proc_t: ProcT,
                  proc_desc: Dict[str, Any] = None) -> List[SvcT]:
    ts = []
    if proc_t is ProcT.hax:
        ts.append(SvcT.M0_CST_HA)
    ts.append(SvcT.M0_CST_RMS)
    if proc_t is ProcT.m0_server:
        assert proc_desc is not None
        if proc_desc.get('runs_confd'):
            ts.append(SvcT.M0_CST_CONFD)
        if proc_desc['_io_disks']:
            ts.extend([SvcT.M0_CST_IOS,
                       SvcT.M0_CST_SNS_REP,
                       SvcT.M0_CST_SNS_REB,
                       SvcT.M0_CST_ADDB2,
                       SvcT.M0_CST_CAS,
                       SvcT.M0_CST_ISCS])
    return ts


class ConfService(ToDhall):
    _objt = ObjT.service
    _downlinks = {Downlink('sdevs', ObjT.sdev)}

    def __init__(self, stype: SvcT, endpoint: Endpoint, sdevs: List[Oid]):
        self.type = stype
        self.endpoint = endpoint
        assert all(x.type is ObjT.sdev for x in sdevs)
        self.sdevs = sdevs

    def __repr__(self):
        args = ', '.join([f'stype={self.type}',
                          f'endpoint={self.endpoint}',
                          f'sdevs={self.sdevs}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.service
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'type = {self.type.to_dhall()}',
                          f'endpoint = {self.endpoint.to_dhall()}',
                          f'sdevs = {oids_to_dhall(self.sdevs)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              stype: SvcT, endpoint: Endpoint) -> Oid:
        assert parent.type is ObjT.process
        svc_id = new_oid(ObjT.service)
        m0conf[svc_id] = cls(stype, endpoint, sdevs=[])
        m0conf[parent].services.append(svc_id)
        return svc_id


class ConfSdev(ToDhall):
    _dev_idx = _infinite_counter()
    _objt = ObjT.sdev
    _downlinks: Set[Downlink] = set()

    def __init__(self, dev_idx: int, filename: str, size: int, blksize: int):
        assert dev_idx >= 0
        assert filename
        assert size >= 0
        assert blksize >= 0

        self.dev_idx = dev_idx
        self.filename = filename
        self.size = size
        self.blksize = blksize

    def __repr__(self):
        args = ', '.join([f'dev_idx={self.dev_idx}',
                          f'filename={self.filename}',
                          f'size={self.size}',
                          f'blksize={self.blksize}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.sdev
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'dev_idx = {self.dev_idx}',
                          f'filename = "{self.filename}"',
                          f'size = {self.size}',
                          f'blksize = {self.blksize}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              disk_desc: Disk) -> Oid:
        assert parent.type is ObjT.service
        sdev_id = new_oid(ObjT.sdev)
        m0conf[sdev_id] = cls(dev_idx=next(cls._dev_idx),
                              filename=disk_desc.path,
                              size=disk_desc.size,
                              blksize=disk_desc.blksize)
        m0conf[parent].sdevs.append(sdev_id)
        return sdev_id


class ConfSite(ToDhall):
    _objt = ObjT.site
    _downlinks = {Downlink('racks', ObjT.rack)}

    def __init__(self, racks: List[Oid], pvers: List[Oid]):
        assert all(x.type is ObjT.rack for x in racks)
        self.racks = racks
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'racks={self.racks}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.site
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'racks = {oids_to_dhall(self.racks)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid) -> Oid:
        assert parent.type is ObjT.root
        site_id = new_oid(ObjT.site)
        m0conf[site_id] = cls(racks=[], pvers=[])
        assert not m0conf[parent].sites
        m0conf[parent].sites = [site_id]  # NB: a single site
        return site_id


class ConfRack(ToDhall):
    _objt = ObjT.rack
    _downlinks = {Downlink('encls', ObjT.enclosure)}

    def __init__(self, encls: List[Oid], pvers: List[Oid]):
        assert all(x.type is ObjT.enclosure for x in encls)
        self.encls = encls
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'encls={self.encls}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.rack
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'encls = {oids_to_dhall(self.encls)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid) -> Oid:
        assert parent.type is ObjT.site
        rack_id = new_oid(ObjT.rack)
        m0conf[rack_id] = cls(encls=[], pvers=[])
        m0conf[rack_id]._parent = parent
        assert not m0conf[parent].racks
        m0conf[parent].racks = [rack_id]  # NB: a single rack
        return rack_id


class ConfEnclosure(ToDhall):
    _objt = ObjT.enclosure
    _downlinks = {Downlink('ctrls', ObjT.controller)}

    def __init__(self, ctrls: List[Oid], pvers: List[Oid]):
        assert all(x.type is ObjT.controller for x in ctrls)
        self.ctrls = ctrls
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'ctrls={self.ctrls}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.enclosure
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'ctrls = {oids_to_dhall(self.ctrls)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid) -> Oid:
        assert parent.type is ObjT.rack
        encl_id = new_oid(ObjT.enclosure)
        m0conf[encl_id] = cls(ctrls=[], pvers=[])
        m0conf[encl_id]._parent = parent
        m0conf[parent].encls.append(encl_id)
        return encl_id


class ConfController(ToDhall):
    _objt = ObjT.controller
    _downlinks = {Downlink('drives', ObjT.drive)}

    def __init__(self, node: Oid, drives: List[Oid], pvers: List[Oid]):
        assert node.type is ObjT.node
        self.node = node
        assert all(x.type is ObjT.drive for x in drives)
        self.drives = drives
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'node={self.node}',
                          f'drives={self.drives}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.controller
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'node = {oid_to_dhall(self.node)}',
                          f'drives = {oids_to_dhall(self.drives)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid, node: Oid) -> Oid:
        assert parent.type is ObjT.enclosure
        assert node.type is ObjT.node

        ctrl_id = new_oid(ObjT.controller)
        m0conf[ctrl_id] = cls(node, drives=[], pvers=[])
        m0conf[ctrl_id]._parent = parent
        m0conf[parent].ctrls.append(ctrl_id)
        return ctrl_id


class ConfDrive(ToDhall):
    _objt = ObjT.drive
    _downlinks: Set[Downlink] = set()

    def __init__(self, sdev: Oid, pvers: List[Oid]):
        assert sdev.type is ObjT.sdev
        self.sdev = sdev
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'sdev={self.sdev}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.drive
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'sdev = {oid_to_dhall(self.sdev)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid, sdev: Oid) -> Oid:
        assert parent.type is ObjT.controller
        assert sdev.type is ObjT.sdev

        drive_id = new_oid(ObjT.drive)
        m0conf[drive_id] = cls(sdev, pvers=[])
        m0conf[drive_id]._parent = parent
        m0conf[parent].drives.append(drive_id)
        return drive_id


PoolT = Enum('PoolT', 'sns dix md')  # PoolType.dhall


def pool_drives(m0conf: Dict[Oid, Any], ptype: PoolT) -> List[Oid]:
    if ptype == PoolT.dix:
        cas = [(svc_id, ctrl_id)
               for ctrl_id in m0conf if ctrl_id.type is ObjT.controller
               for proc_id in m0conf[m0conf[ctrl_id].node].processes
               for svc_id in m0conf[proc_id].services
               if m0conf[svc_id].type is SvcT.M0_CST_CAS]
        return [ConfDrive.build(m0conf, ctrl_id,
                                ConfSdev.build(m0conf, svc_id,
                                               Disk(path='/dev/null',
                                                    size=1024,
                                                    blksize=1)))
                for (svc_id, ctrl_id) in cas]

    if ptype == PoolT.md:
        return [m0conf[ctrl_id].drives[0]
                for ctrl_id in m0conf if ctrl_id.type is ObjT.controller]

    assert ptype == PoolT.sns
    return [oid for oid in m0conf if oid.type is ObjT.drive]


Failures = NamedTuple('Failures', [('site', int),
                                   ('rack', int),
                                   ('enclosure', int),
                                   ('controller', int),
                                   ('drive', int)])
Failures.__doc__ = """\
Failure tolerance vector.

For a given pool version, the failure tolerance vector reflects how
many devices in each level can be expected to fail whilst still
allowing the remaining disks in that pool version to be read.

For disks, then, note that this will equal the parameter K, where
(N,K,P) is the triple of data units, parity units, pool width for
the pool version.

For controllers, this should indicate the maximum number such that
no failure of that number of controllers can take out more than K
units.  We can put an upper bound on this by considering
floor((nr_encls)/(N+K)), though distributional issues may result
in a lower value.
"""
pver_levels = Failures._fields


class ConfPool(ToDhall):
    _objt = ObjT.pool
    _downlinks = {Downlink('pvers', ObjT.pver)}

    def __init__(self, pvers: List[Oid]):
        assert all(x.type in (ObjT.pver, ObjT.pver_f) for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        return f'{self.__class__.__name__}(pvers={self.pvers})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.pool
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              pool_desc: Dict[str, Any], profile: Oid) -> Oid:
        assert parent.type is ObjT.root
        assert profile.type is ObjT.profile

        pool_id = new_oid(ObjT.pool)
        m0conf[pool_id] = cls(pvers=[])
        m0conf[profile].pools.append(pool_id)
        m0conf[parent].pools.append(pool_id)

        t = PoolT[pool_desc.get('type', 'sns')]
        drives = pool_drives(m0conf, t)
        assert ('data_units' in pool_desc) == ('parity_units' in pool_desc)
        if 'data_units' in pool_desc:
            pd_attrs = PDClustAttrs0(pool_desc['data_units'],
                                     pool_desc['parity_units'])
        else:
            if t == PoolT.dix:
                # For CAS service N must be equal to 1.  CAS records are
                # indivisible pieces of data, the whole CAS record is
                # always stored on one node.
                data_units = 1
            else:
                assert t == PoolT.md
                data_units = len(drives)
            pd_attrs = PDClustAttrs0(data_units=data_units, parity_units=0)
        tolerance = None if t == PoolT.sns else Failures(0, 0, 0, 1, 0)
        pver = ConfPver.build(m0conf, pool_id, pd_attrs, drives, tolerance)
        if t == PoolT.dix:
            m0conf[parent].imeta_pver = pver
        elif t == PoolT.md:
            m0conf[parent].mdpool = pool_id
        else:
            assert t == PoolT.sns
            d = pool_desc.get('allowed_failures')
            for v in gen_allowances(Failures(0, 0, 0, 0, 0) if d is None else
                                    Failures(d['site'],
                                             d['rack'],
                                             d['encl'],
                                             d['ctrl'],
                                             d['disk'])):
                ConfPverF.build(m0conf, pool_id, base=pver, allowance=v)

        return pool_id


def gen_allowances(allowed: Failures) -> List[Failures]:
    """Generate all combinations of failures from CDF's `allowed_failures`
    vector.

    >>> gen_allowances(Failures(0, 0, 0, 0, 2))
    [Failures(0, 0, 0, 0, 1), Failures(0, 0, 0, 0, 2)]
    >>> gen_allowances(Failures(0, 0, 0, 1, 2))
    [Failures(0, 0, 0, 0, 1),
     Failures(0, 0, 0, 0, 2),
     Failures(0, 0, 0, 1, 0),
     Failures(0, 0, 0, 1, 1),
     Failures(0, 0, 0, 1, 2)]
    >>> gen_allowances(Failures(0, 0, 0, 0, 0))
    []
    """
    return [Failures(*v) for v in
            filter(any, itertools.product(*(range(n+1) for n in allowed)))]


# m0_pdclust_attr
PDClustAttrs0 = NamedTuple('PDClustAttrs0',
                           # XXX What about `unit_size` and `seed`?
                           [('data_units', int), ('parity_units', int)])
PDClustAttrs0.__doc__ = 'Motr parity de-clustering layout attributes'


def tolerated_failures(m0conf: Dict[Oid, Any], pd_attrs: PDClustAttrs0,
                       drives: List[Oid]) -> Failures:
    assert pd_attrs.data_units > 0
    assert pd_attrs.parity_units >= 0
    assert drives and all(x.type is ObjT.drive for x in drives)
    assert all_unique(drives)

    n, k = pd_attrs

    nr_ctrls = 0
    for ctrl_id, ctrl in m0conf.items():
        if ctrl_id.type is not ObjT.controller:
            continue
        if not set(m0conf[ctrl_id].drives).isdisjoint(set(drives)):
            nr_ctrls += 1
    assert nr_ctrls > 0

    q, r = divmod(n + 2*k, nr_ctrls)
    # There are `r` controllers with `q+1` units and
    # `(nr_ctrls - r)` controllers with `q` units.
    #
    # In the worst scenario the most populated controllers
    # will go down first.
    #
    # `kc` is the number of parity group units that will be
    # unavailable when those `r` controllers go down.
    kc = r * (q + 1)
    if kc > k:
        # We won't be able to recover data if `kc` units are lost.
        # Recalculate the tolerable number of controller failures
        # by distributing the tolerable number of unit failures
        # (`k`) among the "most populous" controllers (`q+1` units
        # in each).
        ctrl_failures = k // (q + 1)
    elif kc < k:
        # `kc` units (`r` controllers) are lost, but we can
        # tolerate losing `k - kc` more units.
        # `(k - kc) // q` is the number of additional controller
        # failures that we can tolerate.
        ctrl_failures = r + (k - kc) // q
    else:
        # kc == k.  We can lose precisely `r` controllers;
        # no more, no less.
        ctrl_failures = r
    return Failures(site=0, rack=0, enclosure=0, controller=ctrl_failures,
                    drive=k)


class ConfPver(ToDhall):
    _objt = ObjT.pver
    _downlinks = {Downlink('sitevs', ObjT.objv)}

    def __init__(self, data_units: int, parity_units: int, pool_width: int,
                 tolerance: List[int], sitevs: List[Oid]):
        assert data_units > 0
        assert parity_units >= 0
        assert pool_width > 0
        assert len(tolerance) == len(pver_levels)
        assert all(n >= 0 for n in tolerance)
        assert all(x.type is ObjT.objv for x in sitevs)

        self.data_units = data_units
        self.parity_units = parity_units
        self.pool_width = pool_width
        self.tolerance = tolerance
        self.sitevs = sitevs

    def __repr__(self):
        args = ', '.join([f'data_units={self.data_units}',
                          f'parity_units={self.parity_units}',
                          f'pool_width={self.pool_width}',
                          f'tolerance={self.tolerance}',
                          f'sitevs={self.sitevs}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.pver
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'data_units = {self.data_units}',
                          f'parity_units = {self.parity_units}',
                          f'pool_width = {self.pool_width}',
                          f'tolerance = {self.tolerance}',
                          f'sitevs = {oids_to_dhall(self.sitevs)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              pd_attrs: PDClustAttrs0, drives: List[Oid],
              tolerance: Failures = None) -> Oid:
        assert parent.type is ObjT.pool
        assert drives and all(x.type is ObjT.drive for x in drives)
        assert all_unique(drives)

        pver_id = new_oid(ObjT.pver)  # "base" pool version
        m0conf[pver_id] = cls(
            data_units=pd_attrs.data_units,
            parity_units=pd_attrs.parity_units,
            pool_width=len(drives),
            tolerance=list(tolerated_failures(m0conf, pd_attrs, drives)
                           if tolerance is None else tolerance),
            sitevs=[])
        m0conf[parent].pvers.append(pver_id)
        build_pver_subtree(m0conf, pver_id, drives)
        return pver_id


class ConfPverF(ToDhall):
    _cuid = _infinite_counter()  # cluster-unique pver_f identifier
    _objt = ObjT.pver_f
    _downlinks: Set[Downlink] = set()

    def __init__(self, cuid: int, base: Oid, allowance: List[int]):
        assert cuid >= 0
        assert base.type is ObjT.pver
        assert len(allowance) == len(pver_levels)
        all(n >= 0 for n in allowance)

        self.cuid = cuid
        self.base = base
        self.allowance = allowance

    def __repr__(self):
        args = ', '.join([f'cuid={self.cuid}',
                          f'base={self.base}',
                          f'allowance={self.allowance}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.pver_f
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'cuid = {self.cuid}',
                          f'base = {oid_to_dhall(self.base)}',
                          f'allowance = {self.allowance}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              base: Oid, allowance: Failures) -> Oid:
        assert parent.type is ObjT.pool

        pver_id = new_oid(ObjT.pver_f)
        m0conf[pver_id] = cls(cuid=next(cls._cuid),
                              base=base,
                              allowance=list(allowance))
        m0conf[parent].pvers.append(pver_id)
        return pver_id


def build_pver_subtree(m0conf: Dict[Oid, Any], pver: Oid,
                       drives: List[Oid]) -> None:
    assert pver.type is ObjT.pver
    assert drives and all(x.type is ObjT.drive for x in drives)

    # Maps real object to the corresponding virtual object.
    virtual: Dict[Oid, Oid] = {}

    for drive in drives:
        assert drive not in virtual
        assert pver not in m0conf[drive].pvers
        m0conf[drive].pvers.append(pver)
        virtual[drive] = ConfObjv.build(m0conf, real=drive)

        devs = [drive]
        parent_types = [getattr(ObjT, x) for x in pver_levels[:-1]]
        while parent_types:
            devs.insert(0, m0conf[devs[0]]._parent)
            dev = devs[0]
            assert dev.type is parent_types.pop()
            parent_ready = dev in virtual
            if not parent_ready:
                assert pver not in m0conf[dev].pvers
                m0conf[dev].pvers.append(pver)
                virtual[dev] = ConfObjv.build(m0conf, real=dev)
            m0conf[virtual[dev]].children.append(virtual[devs[1]])
            if parent_ready:
                break

        if not parent_types:
            m0conf[pver].sitevs.append(virtual[devs[0]])


class ConfObjv(ToDhall):
    _objt = ObjT.objv
    _downlinks = {Downlink('children', ObjT.objv)}

    def __init__(self, real: Oid, children: List[Oid]):
        assert real.type in [getattr(ObjT, x) for x in pver_levels]
        self.real = real
        assert all(x.type is ObjT.objv for x in children)
        self.children = children

    def __repr__(self):
        args = ', '.join([f'real={self.real}',
                          f'children={self.children}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.objv
        args = ', '.join([f'id={oid_to_dhall(oid)}',
                          f'real={oid_to_dhall(self.real)}',
                          f'children={oids_to_dhall(self.children)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], real: Oid) -> Oid:
        assert real in m0conf
        objv_id = new_oid(ObjT.objv)
        m0conf[objv_id] = cls(real, [])
        return objv_id


class ConfProfile(ToDhall):
    _objt = ObjT.profile
    _downlinks = {Downlink('pools', ObjT.pool)}

    def __init__(self, pools: List[Oid]):
        assert all(x.type is ObjT.pool for x in pools)
        self.pools = pools

    def __repr__(self):
        return f'{self.__class__.__name__}(pools={self.pools})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.profile
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'pools = {oids_to_dhall(self.pools)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid) -> Oid:
        assert parent.type is ObjT.root
        prof_id = new_oid(ObjT.profile)
        m0conf[prof_id] = cls(pools=[])
        assert not m0conf[parent].profiles
        m0conf[parent].profiles = [prof_id]  # XXX-TODO: multiple profiles
        return prof_id


ConsulAgent = NamedTuple('ConsulAgent', [('node_name', str), ('ipaddr', str)])

Cluster = NamedTuple('Cluster', [('m0conf', Dict[Oid, Any]),
                                 ('consul_servers', List[ConsulAgent]),
                                 ('consul_clients', List[ConsulAgent]),
                                 ('m0_clients', Dict[Oid, Oid])])


def generate_consul_agents(cluster: Cluster) -> str:
    assert cluster.consul_servers
    return json.dumps(
        {'servers': [x._asdict() for x in cluster.consul_servers],
         'clients': [x._asdict() for x in cluster.consul_clients]},
        indent=2) + '\n'


def fid2str(id: Oid) -> str:
    types = {ObjT.root: 't', ObjT.fdmi_flt_grp: 'g', ObjT.fdmi_filter: 'l',
             ObjT.node: 'n', ObjT.process: 'r', ObjT.service: 's',
             ObjT.sdev: 'd', ObjT.pool: 'o', ObjT.pver: 'v', ObjT.objv: 'j',
             ObjT.site: 'S', ObjT.rack: 'a', ObjT.enclosure: 'e',
             ObjT.controller: 'c', ObjT.drive: 'k', ObjT.profile: 'p'}
    return f'{hex((ord(types[id.type]) << 56)+1)}:{hex(id.fidk)}'


def generate_consul_kv(cluster: Cluster, dhall_dir: str) -> str:
    assert os.path.isabs(dhall_dir)
    assert all(k.type is ObjT.process and v.type is ObjT.service
               for k, v in cluster.m0_clients.items())

    global fidk_gen
    _fidk_gen = next(fidk_gen)
    # Give up ownership of `fidk_gen`, passing it to the Consul KV.
    del fidk_gen

    m0conf = cluster.m0conf

    ConsulService = NamedTuple('ConsulService', [
        ('node_name', str), ('proc_id', Oid), ('ep', Endpoint),
        ('svc_id', Oid), ('stype', str), ('meta_data', Optional[str])])

    def profile() -> str:
        profiles = [x for x in m0conf.keys() if x.type is ObjT.profile]
        assert len(profiles) != 0
        # Need to return all when it support multiple profiles
        return fid2str(profiles[0])

    def sns_pools() -> str:
        for root_id, root in m0conf.items():
            if root_id.type is ObjT.root:
                return ' '.join([fid2str(x) for x in root.pools
                                 if x is not root.mdpool and
                                 root.imeta_pver not in m0conf[x].pvers])
        raise RuntimeError('Impossible happened')

    def processes() -> Iterator[ConsulService]:
        for node_id, node in m0conf.items():
            if node_id.type is not ObjT.node:
                continue
            for proc_id in node.processes:
                assert proc_id.type is ObjT.process
                for svc_id in m0conf[proc_id].services:
                    stype = m0conf[svc_id].type.name[len('M0_CST_'):].lower()
                    proc = m0conf[proc_id]
                    yield ConsulService(node_name=node.name,
                                        proc_id=proc_id,
                                        ep=proc.endpoint,
                                        svc_id=svc_id,
                                        stype=stype,
                                        meta_data=proc.meta_data)
                # Add Motr clients to consul kv
                if proc_id in cluster.m0_clients:
                    proc_ep = m0conf[proc_id].endpoint
                    yield ConsulService(node_name=node.name,
                                        proc_id=proc_id,
                                        ep=proc.endpoint,
                                        svc_id=cluster.m0_clients[proc_id],
                                        stype=ProcT(proc_ep.portal).name,
                                        meta_data=None)

    def drives() -> List:
        site_drives = []
        for site_id, site in m0conf.items():
            if site_id.type is not ObjT.site:
                continue
            site_key = f'm0conf/sites/{fid2str(site_id)}'
            site_val = json.dumps({'state': 'M0_NC_UNKNOWN'})
            site_drives.append((site_key, site_val))
            for rack_id in site.racks:
                assert rack_id.type is ObjT.rack
                rack = m0conf[rack_id]
                rack_key = f'{site_key}/racks/{fid2str(rack_id)}'
                rack_val = json.dumps({'state': 'M0_NC_UNKNOWN'})
                site_drives.append((rack_key, rack_val))
                for encl_id in rack.encls:
                    assert encl_id.type is ObjT.enclosure
                    encl = m0conf[encl_id]
                    encl_key = f'{rack_key}/encls/{fid2str(encl_id)}'
                    encl_val = json.dumps({'state': 'M0_NC_UNKNOWN'})
                    site_drives.append((encl_key, encl_val))
                    for ctrl_id in encl.ctrls:
                        assert ctrl_id.type is ObjT.controller
                        ctrl = m0conf[ctrl_id]
                        ctrl_key = f'{encl_key}/ctrls/{fid2str(ctrl_id)}'
                        ctrl_val = json.dumps({'node': fid2str(ctrl.node),
                                               'state': 'M0_NC_UNKNOWN'})
                        site_drives.append((ctrl_key, ctrl_val))
                        for drive_id in ctrl.drives:
                            assert drive_id.type is ObjT.drive
                            drive = m0conf[drive_id]
                            key = f'{ctrl_key}/drives/{fid2str(drive_id)}'
                            val = json.dumps({'sdev': fid2str(drive.sdev),
                                              'state': 'M0_NC_UNKNOWN'})
                            site_drives.append((key, val))
        return site_drives

    def sdevs() -> List:
        node_sdevs = []
        for node_id, node in m0conf.items():
            if node_id.type is not ObjT.node:
                continue
            node_key = f'm0conf/nodes/{fid2str(node_id)}'
            node_val = json.dumps({'name': node.name,
                                   'state': 'M0_NC_UNKNOWN'})
            node_sdevs.append((node_key, node_val))
            for proc_id in node.processes:
                assert proc_id.type is ObjT.process
                proc = m0conf[proc_id]
                proc_key = f'{node_key}/processes/{fid2str(proc_id)}'
                proc_name = ProcT(proc.endpoint.portal).name
                proc_val = json.dumps({'name': proc_name,
                                       'state': 'M0_NC_UNKNOWN'})
                node_sdevs.append((proc_key, proc_val))
                for svc_id in m0conf[proc_id].services:
                    stype = m0conf[svc_id].type.name[len('M0_CST_'):].lower()
                    svc_key = f'{proc_key}/services/{fid2str(svc_id)}'
                    svc_val = json.dumps({'name': stype,
                                          'state': 'M0_NC_UNKNOWN'})
                    node_sdevs.append((svc_key, svc_val))
                    for sdev_id in m0conf[svc_id].sdevs:
                        assert sdev_id.type is ObjT.sdev
                        sdev = m0conf[sdev_id]
                        disk_key = f'{svc_key}/sdevs/{fid2str(sdev_id)}'
                        disk_val = json.dumps({'path': sdev.filename,
                                               'state': 'M0_NC_UNKNOWN'})
                        node_sdevs.append((disk_key, disk_val))
        return node_sdevs

    node_sdevs = [dict(key=k, value=v) for k, v in sdevs()]
    site_drives = [dict(key=k, value=v) for k, v in drives()]
    return json.dumps([dict(key=k, value=v) for k, v in (
        ('epoch', 1),
        ('eq-epoch', 1),
        ('leader', ''),
        ('last_fidk', _fidk_gen),
        ('m0conf/profiles', profile()),
        ('m0conf/profiles/pools', sns_pools()),
        *[(f'm0conf/nodes/{x.node_name}/processes/{x.proc_id.fidk}/services/'
           f'{x.stype}', x.svc_id.fidk) for x in processes()],
        *[(f'm0conf/nodes/{x.node_name}/processes/{x.proc_id.fidk}/'
           f'meta_data', x.meta_data)
          for x in processes() if x.stype == 'ios' and x.meta_data],
        *[(f'm0conf/nodes/{x.node_name}/processes/{x.proc_id.fidk}/endpoint',
           str(x.ep))
          for x in processes()])] + node_sdevs + site_drives,
                      indent=2) + '\n'


def generate_confd(m0conf: Dict[Oid, ToDhall], dhall_dir: str) -> str:
    assert os.path.isabs(dhall_dir)

    def objt(obj: ToDhall) -> str:
        objt = obj.__class__.__name__
        assert objt.startswith('Conf')
        return objt[len('Conf'):]

    objs = '\n  , '.join('types.Obj.{} {}'.format(objt(v), v.to_dhall(k))
                         for k, v in m0conf.items())
    return f"""\
let types = {dhall_dir}/types.dhall
let utils = {dhall_dir}/utils.dhall

let objs =
  [ {objs}
  ]

in {dhall_dir}/toConfGen.dhall objs
"""


def validate_m0conf(m0conf: Dict[Oid, Any]) -> None:
    children_by_parent_type: Dict[ObjT, Dict[Downlink, List[Oid]]] = \
        dict((parent_t, {}) for parent_t in ObjT)
    for oid, obj in m0conf.items():
        assert oid.type is obj._objt
        assert obj.__class__.__name__ == \
            'Conf' + ''.join(s.capitalize() for s in oid.type.name.split('_'))
        for rel in obj._downlinks:
            children = getattr(obj, rel.name)
            assert all_unique(children)
            assert all(
                (x.type is rel.children_t or
                 # XXX leaky abstraction
                 (x.type is ObjT.pver_f and rel.children_t is ObjT.pver))
                for x in children)
            children_by_parent_type[oid.type].setdefault(rel.name, []) \
                                             .extend(children)
    assert all(all_unique(children)
               for _parent_t, rel_children in children_by_parent_type.items()
               for _rel, children in rel_children.items())


def build_cluster(cluster_desc: Dict[str, Any]) -> Cluster:
    cluster = Cluster(m0conf={}, consul_servers=[], consul_clients=[],
                      m0_clients={})
    conf = cluster.m0conf
    root_id = ConfRoot.build(conf)
    # XXX Move all the logic into ConfRoot.build?

    rack_id = ConfRack.build(conf, ConfSite.build(conf, root_id))

    for node in cluster_desc['nodes']:
        node_id = ConfNode.build(conf, root_id, node)
        ctrl_id = None
        if any(m0d['_io_disks'] for m0d in node['m0_servers']):
            ctrl_id = ConfController.build(conf,
                                           ConfEnclosure.build(conf, rack_id),
                                           node_id)

        ConfProcess.build(conf, node_id, node, ProcT.hax)

        confd_p = False
        for m0d in node['m0_servers']:
            ConfProcess.build(conf, node_id, node, ProcT.m0_server, m0d,
                              ctrl_id)
            if m0d['runs_confd']:
                confd_p = True

        ipaddr = node['facts'][ipaddr_key(node['data_iface'])]
        (cluster.consul_servers if confd_p else cluster.consul_clients).\
            append(ConsulAgent(node_name=node['hostname'],
                               # XXX use 'mgmt_iface' for consul?
                               ipaddr=ipaddr))

        for client_t, proc_t in [('s3', ProcT.m0_client_s3),
                                 ('other', ProcT.m0_client_other)]:
            for _ in range(node['m0_clients'][client_t]):
                proc_id = ConfProcess.build(conf, node_id, node, proc_t)
                cluster.m0_clients[proc_id] = new_oid(ObjT.service)

    # XXX-TODO: support multiple profiles
    prof_id = ConfProfile.build(conf, root_id)

    for pool in cluster_desc['pools']:
        ConfPool.build(conf, root_id, pool, prof_id)

    if conf[root_id].imeta_pver is None:  # no DIX pool defined in the CDF
        ConfPool.build(conf, root_id, {'type': 'dix'}, prof_id)

    if conf[root_id].mdpool is None:  # no Metadata pool defined in the CDF
        ConfPool.build(conf, root_id, {'type': 'md'}, prof_id)

    validate_m0conf(conf)
    assert cluster.consul_servers
    names = [x.node_name
             for x in cluster.consul_servers + cluster.consul_clients]
    assert all_unique(names)
    assert all(re.search('[",= @]', name) is None for name in names)
    assert all_unique(x.ipaddr
                      for x in cluster.consul_servers + cluster.consul_clients)
    return cluster


if __name__ == '__main__':
    main()
