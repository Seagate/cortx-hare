#!/usr/bin/env python3

import abc
import argparse
import ast
from enum import Enum, auto, IntEnum
from functools import lru_cache, wraps
import itertools
import json
import os
from pprint import pprint
from logging import StreamHandler
from logging.handlers import RotatingFileHandler
import psutil
import random
import re
import socket
import subprocess
import sys
from typing import (Any, Callable, Dict, Iterable, Iterator, List, NamedTuple,
                    Optional, Set, Type, Tuple)
from math import floor, ceil
from hax.log import create_logger_directory
from helper.exec import CliException, Executor, Program
import yaml
import logging

from dataclasses import dataclass


# This class will hold info for disks mentioned in CDF file
# One use case is to generate list of all disks present in CDF
# file
@dataclass
class DiskRef():
    node: str
    path: str


__version__ = '0.14'

faultToleranceInfo = NamedTuple('faultToleranceInfo', [
                            ('max_drives_per_node', int),
                            ('max_drives_per_ctrl', int),
                            ('min_drives_per_node', int)])


# Global variable aux_pools is used for storing aux pools list
# Example of aux pool variable info :
# {'the pool':
#   [
#     {
#       'name': 'the pool-aux001',
#       'data_units': 2,
#       'parity_units': 2,
#       'spare_units': 0,
#       'disk_refs': [
#         {'path': '/dev/sdb', 'node': 'ssc-vm-6327.colo.seagate.com'},
#         {'path': '/dev/sdc', 'node': 'ssc-vm-6327.colo.seagate.com'},
#         {'path': '/dev/sdb', 'node': 'ssc-vm-2493.colo.seagate.com'},
#         {'path': '/dev/sdc', 'node': 'ssc-vm-2493.colo.seagate.com'}
#       ]
#     },
#     { .. Similarly for 'the pool-aux002' and so on.
#     }
#   ]
# }
aux_pools: Dict[str, List[Dict[str, List[Dict[str, Any]]]]] = {}


def repeat_on_cmd_timeout(max_retries=-1):
    """
    This is a decorator for command timeouts.

    Ensures that the wrapped function gets re-invoked if the exception
    subprocess.TimeoutExpired gets raised. In other words, this wrapper
    makes the wrapped function repeatable on getting command timeout.

    Parameters:
    f - function to be decorated (repeated)
    max_retries - how many attempts the wrapper will perform until finally
         re-raising the exception. -1 means 'repeat forever'.
    """
    def callable(f):
        @wraps(f)
        def wrapper(*args, **kwds):
            attempt_count = 0
            while (True):
                try:
                    return f(*args, **kwds)

                except subprocess.TimeoutExpired as e:
                    attempt_count += 1
                    if max_retries >= 0 and attempt_count > max_retries:
                        logging.warn(
                            '\nFunction %s: Too many errors happened in a row'
                            ' (max_retries = %d)', f.__name__, max_retries)
                        raise e
                    logging.warn(
                        f'\nGot {e.timeout} sec timeout '
                        f'for function {f.__name__} '
                        f'(attempt {attempt_count}). The attempt will '
                        f'be repeated again.'
                        f'\nProbable cause : degraded machine '
                        f'performance or poor network.')

        return wrapper

    return callable


def parse_opts(argv):
    p = argparse.ArgumentParser(description='Generate configuration files'
                                ' required to start Motr cluster.',
                                usage='%(prog)s [OPTION]... CDF')
    p.add_argument('CDF', type=argparse.FileType(),
                   help='cluster description file;'
                   " use '--help-schema' option for format description")
    p.add_argument('--help-schema', nargs=0,
                   help='show the schema of cluster description file (CDF)',
                   action=ShowSchema)
    default_dhall_dir = '/opt/seagate/cortx/hare/share/cfgen/dhall'
    p.add_argument('-D', '--dhall', metavar='dir',
                   help='directory with auxiliary Dhall expressions'
                   f' (defaults to {default_dhall_dir!r})',
                   dest='dhall_dir', default=default_dhall_dir)
    p.add_argument('-l',
                   metavar='log-dir',
                   help='directory to write the logs to. If not specified, '
                   'logs will be not be written to any file',
                   dest='log_dir',
                   default='')
    p.add_argument('--log-file',
                   help='File name of the log files (applicable only when '
                   'combined with -l parameter)',
                   dest='log_file',
                   default='cfgen.log')
    p.add_argument('-v',
                   help="be verbose (DEBUG messages will be seen)",
                   dest='verbose',
                   action='store_true',
                   default=False)
    p.add_argument('-o', metavar='output-dir',
                   help="output directory (defaults to '.')",
                   dest='output_dir', default='.')
    p.add_argument('--mock', help='Generate pseudo-random "facts". The hosts'
                   ' specified in the cluster description file will not be'
                   " visited and don't even have to exist.",
                   action='store_true')
    p.add_argument('--debug',
                   help='print the enriched cluster description and exit',
                   action='store_true')
    p.add_argument('-V', '--version', action='version',
                   version='%(prog)s ' + __version__)
    opts = p.parse_args(argv)

    # Sanity check.
    dirs = [('--dhall', opts.dhall_dir, os.R_OK)]
    if not opts.debug:
        dirs.append(('-o', opts.output_dir, os.W_OK))
    for opt, d, perm in dirs:
        if not (d and os.path.isdir(d) and os.access(d, os.X_OK | perm)):
            what = 'read' if perm == os.R_OK else 'writ'
            die(f'{opt!r} argument must be a path to {what}able directory. '
                f'Current value: {d}')

    opts.dhall_dir = os.path.abspath(opts.dhall_dir)
    return opts


class ShowSchema(argparse.Action):
    def __call__(self, *args):
        # XXX-TODO: Rename `nodes.hostname` to `nodes.name`.
        print("""\
# Cluster Description is a YAML file with the following schema:
---  # start of the document (optional)
create_aux: <bool> # optional, defaults to false if not present in CDF
nodes:
  - hostname: <str>    # [user@]hostname; e.g., localhost, pod-c1
    data_iface: <str>  # name of network interface; e.g., eth1, eth1:c1
    data_iface_type: tcp|o2ib  # type of network interface;
                               # optional, defaults to "tcp"
    m0_servers:        # optional for client-only nodes
      - runs_confd: <bool>  # optional, defaults to false
        io_disks:
          meta_data: <str>  # device path for meta-data;
                            # optional, Motr will use "/var/motr/m0d-<FID>/"
                            # by default
          data: [ <str> ]   # e.g. [ "/dev/loop0", "/dev/loop1", "/dev/loop2" ]
                            # Empty list means no IO service.
    m0_clients:
        s3: <int>     # number of S3 servers to start
        other: <int>  # max quantity of other Motr clients this host may have
pools:
  - name: <str>
    type: sns|dix|md   # optional, defaults to "sns";
                       # "sns" - data pool, "dix" - KV, "md" - meta-data pool.
    disk_refs:  # optional section
      - path: <str>  # io_disks.data value
        node: <str>  # optional; 'hostname' of the corresponding node
    data_units: <int>
    parity_units: <int>
    spare_units: <int>
    allowed_failures:  # optional section; no failures will be allowed
                       # if this section is missing or all of its elements
                       # are zeroes
      site: <int>
      rack: <int>
      encl: <int>
      ctrl: <int>
      disk: <int>

# Profile is a reference to pools.  When Motr client is started, it receives
# profile fid from the command line.  Motr client can only use the pools
# referred to by its profile.
#
profiles:  # This section is optional.  If it is missing, a single "default"
           # profile referring to all pools will be created.
  - name: <str>
    pools: [ <str> ]

# FDMI filter defines if an FDMI record would be sent from FDMI source to FDMI
# plugin. Only M0_FDMI_FILTER_TYPE_KV_SUBSTRING filters are supported.
#
fdmi_filters:  # This section is optional.  If it is missing, no fdmi filters
               # would be defined
  - name: <str>  # Human-readable name of the filter
    node: <str>  # Hostname of the node that hosts FDMI plugin
                 # that would receive every FDMI record this filter matches
    client_index: <int>  # 0-based index of the client that would be
                         # the FDMI plugin. Only "other" clients could be FDMI
                         # plugins. "s3" clients are not used to for this
                         # index.
                         # The client has to be defined in m0_clients section
                         # already.
    substrings: [ <str> ]  # List of strings for FDMI plugin to match
                           # See m0_conf_fdmi_filter::ff_substrings
                           # for the reference. May be empty or absent.
...  # end of the document (optional)""")
        sys.exit()


def setup_logging(opts: Any) -> None:
    log_dir = opts.log_dir
    max_size = 1024 * 1024
    handlers: List[logging.Handler] = [StreamHandler(stream=sys.stderr)]
    if log_dir:
        filename = opts.log_file
        log_file = f'{log_dir}/{filename}'
        create_logger_directory(log_dir)
        handlers.append(
            RotatingFileHandler(log_file,
                                maxBytes=max_size,
                                mode='a',
                                backupCount=5,
                                encoding=None,
                                delay=False))

    verbosity = logging.INFO
    if opts.verbose:
        verbosity = logging.DEBUG

    logging.basicConfig(level=verbosity,
                        handlers=handlers,
                        format='%(asctime)s [%(levelname)s] %(message)s')


def main(argv=None):
    opts = parse_opts(argv)
    setup_logging(opts)

    try:
        check_dhall_versions()

        cdf = opts.CDF.read()
        opts.CDF.close()
        validate_cdf_schema(cdf,
                            cdf_path=opts.CDF.name,
                            schema_path=os.path.join(
                                opts.dhall_dir, 'types', 'ClusterDesc.dhall'))
        cluster_desc = yaml.safe_load(cdf)

        enrich_cluster_desc(cluster_desc, opts.mock)
        validate_cluster_desc(cluster_desc)

        if opts.debug:
            pprint(cluster_desc)
            return

        cluster = build_cluster(cluster_desc)

        outs: List[Tuple[str, Callable[..., str], List[Any]]] = [
            ('consul-agents.json', generate_consul_agents, [cluster]),
            ('consul-kv.json', generate_consul_kv, [cluster, opts.dhall_dir]),
            ('confd.dhall', generate_confd, [cluster.m0conf, opts.dhall_dir])]
        for path, generate, args in outs:
            with open(os.path.join(opts.output_dir, path), 'w') as f:
                f.write(generate(*args))
    except Exception as e:
        logging.error('Exiting with FAILURE status. Reason: %s', e)
        logging.debug('Stacktrace can be seen below', exc_info=True)
        sys.exit(1)


def die(msg: str):
    raise RuntimeError(msg)


def all_not_none(xs: Iterable) -> bool:
    """Returns True iff the iterable contains no None elements.
    """
    for t in xs:
        if t is None:
            return False
    return True


def all_unique(xs) -> bool:
    """Returns True iff all entries of the sequence are unique.
    """
    if hasattr(xs, '__iter__') and hasattr(xs, '__next__'):
        # `xs` is a generator.  We should not consume it twice.
        xs = list(xs)
    return len(xs) == len(set(xs))


Version = NamedTuple('Version',
                     [('major', int), ('minor', int), ('patch', int)])


def version(s: str) -> Version:
    # We could have used `packaging.version` module, but it's not
    # worth to add an external dependency.
    return Version(*map(int, s.split('.', 2)))


def check_dhall_versions() -> None:
    # See https://github.com/dhall-lang/dhall-haskell/releases
    logging.debug('Checking dhall versions')
    versions = {'dhall': version('1.26.1'), 'yaml-to-dhall': version('1.4.1')}

    for exe, ver in versions.items():
        try:
            out = Executor().run(Program([exe, '--version']), timeout=15)
        except CliException:
            die(f'{exe} >= {ver} required, none found')

        if version(out.strip()) < ver:
            die(f'{exe} >= {ver} required')


def _assert(expr: bool,
            message: str,
            exc_type: Type[BaseException] = AssertionError):
    if not expr:
        raise exc_type(message)


def validate_cdf_schema(cdf: str, cdf_path: str, schema_path: str) -> None:
    logging.debug('Validating the provided CDF file structure')
    _assert(os.path.isabs(schema_path),
            f'Dhall schema is not found at path {schema_path}')

    try:
        Executor().run(Program(['yaml-to-dhall', schema_path]),
                       timeout=15,
                       input=cdf)
    except CliException as e:
        die(
            f'{cdf_path}: Invalid cluster description\n' + e.stderr +
            f"\n\nTo learn more, run '{sys.argv[0]} --help-schema'.")


def ipaddr_key(iface: str) -> str:
    return 'ipaddress_' + iface


def enrich_cluster_desc(desc: Dict[str, Any], mock_p: bool) -> None:
    logging.debug('Enriching cluster description')
    for node in desc['nodes']:
        # The fact names used here correspond to version 2.4.1 of `facter`.
        # See https://puppet.com/docs/puppet/6.6/core_facts.html#legacy-facts

        host = node['hostname']
        if all(key in node.keys()
               for key in ('processorcount', 'memorysize_mb')):

            logging.debug(
                'Cluster description already has facts for '
                '[hostname=%s]. facter tool will not be used', host)
            node['facts'] = {
                '_memsize_MB': int(float(node['memorysize_mb'])),
                'processorcount': node['processorcount'],
                ipaddr_key(node['data_iface']): node['data_iface_ip_addr']
            }
        else:
            node['facts'] = get_facts(host, mock_p, 'processorcount',
                                      'memorysize_mb',
                                      ipaddr_key(node['data_iface']))
            node['facts']['_memsize_MB'] = int(
                float(node['facts']['memorysize_mb']))

        logging.debug('Node [hostname=%s] facts=%s', host, node['facts'])
        if 'm0_servers' not in node:
            continue

        m0d = node.get('m0_servers')
        if m0d is None:
            continue
        for m0d in node['m0_servers']:
            m0d.setdefault('runs_confd', False)
            if 'meta_data' in m0d['io_disks']:
                if m0d['io_disks']['meta_data'] is None:
                    del m0d['io_disks']['meta_data']
            m0d['_io_disks'] = get_disks_from_cdf(node['hostname'], mock_p,
                                                  m0d['io_disks']['data'])

            if not m0d['_io_disks']:
                m0d['_io_disks'] = get_disks_ssh(node['hostname'], mock_p,
                                                 m0d['io_disks']['data'])

    if 'profiles' not in desc:
        sns_pools = [pool['name'] for pool in desc['pools']
                     if pool_type(pool) is PoolT.sns]
        desc['profiles'] = [{'name': 'default', 'pools': sns_pools}]

    portalgroup = PortalGroup
    if 'network_ports' not in desc or not bool(desc['network_ports']):
        desc['network_ports'] = {port_type.name: port_type.value
                                 for port_type in portalgroup}
    else:
        for key, value in desc['network_ports'].items():
            if value is None:
                desc['network_ports'][key] = portalgroup[key].value
        for srv in [port_type.name for port_type in portalgroup]:
            if srv not in desc['network_ports']:
                desc['network_ports'][srv] = portalgroup[srv].value


def validate_cluster_desc(desc: Dict[str, Any]) -> None:
    validate_nodes_desc(desc['nodes'])
    validate_pools_desc(desc['pools'])
    if 'profiles' in desc:
        validate_profiles_desc(desc['profiles'], desc['pools'])
    if 'fdmi_filters' in desc and desc['fdmi_filters'] is not None:
        validate_fdmi_filters_desc(desc['fdmi_filters'], desc['nodes'])
    logging.debug('Validations passed')


def validate_nodes_desc(nodes_desc: List[Dict[str, Any]]) -> None:
    logging.debug('Validating node descriptions')
    Node = NamedTuple('Node', [('name', str), ('ipaddr', str)])
    nodes = [Node(name=x['hostname'],
                  ipaddr=x['facts'][ipaddr_key(x['data_iface'])])
             for x in nodes_desc]
    _assert(all_not_none(x.name for x in nodes),
            f'Unexpected None hostname found in list: {nodes}\n'
            f'Full context: {nodes_desc}')
    _assert(all_unique(x.name for x in nodes),
            "Node names ('hostname' values in the CDF) are not unique:\n" +
            '\n'.join('  ' + x.name for x in nodes))
    _assert(all_not_none(x.ipaddr for x in nodes),
            f'Unexpected None ipaddr found in list: {nodes}\n'
            f'Full context: {nodes_desc}')
    _assert(all_unique(x.ipaddr for x in nodes),
            'IP addresses are not unique:\n' +
            '\n'.join('  ' + x.ipaddr for x in nodes))

    total_nr_confds = total_nr_disks = 0

    for i, node in enumerate(nodes_desc):
        name = node['hostname']
        _assert(name, f'Hostname not set. Node index in CDF = {i}')
        iface = node['data_iface']
        _assert(iface, f'iface not set at hostname = {name}')
        _assert(node['facts'][ipaddr_key(iface)],
                f"""{name}: {iface!r} interface has no IP address
Make sure the value of data_iface in the CDF is correct.""")
        m0d = node.get('m0_servers')
        if m0d is None:
            continue

        if 'm0_servers' not in node:
            _assert((node['m0_clients']['s3']
                    + node['m0_clients']['other'] > 0),
                    f'Node {name}: At least one client should be configured')
            continue

        nr_confds = sum(1 for m0d in node['m0_servers'] if m0d['runs_confd'])
        _assert(nr_confds < 2,
                f'Node {name}: Too many confd services: {nr_confds} >= 2')
        total_nr_confds += nr_confds

        for m0d in node['m0_servers']:
            d = m0d['io_disks']
            _assert(m0d['runs_confd'] or d['data'],
                    f"Node {name}: Either 'runs_confd' or"
                    "'io_disks.data' must be set")
            _assert('' not in d['data'], f"Node {name}: Empty strings "
                    "in 'io_disks.data' are not allowed")
            if 'meta_data' in d:
                _assert(d['meta_data'], f"Node {name}: 'io_disks.meta_data'"
                        'must not be an empty string')
                _assert(d['meta_data'] not in d['data'], f"Node {name}: "
                        'Meta-data disk must not belong io_disks.data')

        disks: List[Disk] = []
        for m0d in node['m0_servers']:
            disks.extend(m0d['_io_disks'])
        _assert(all_unique(disks),
                f'Node {name}: The same disk is used by several IO services')
        total_nr_disks += len(disks)

        _assert(
            nr_confds + node['m0_clients']['s3'] + node['m0_clients']['other']
            > 0, f'Node {name}: At least one Motr server or '
            'client is required')

    _assert(total_nr_confds > 0, 'At least one confd is required')
    _assert(total_nr_disks > 0, 'No disks found')


def validate_pools_desc(pools_desc: List[Dict[str, Any]]) -> None:
    logging.debug('Validating pool descriptions')
    pool_names = [x['name'] for x in pools_desc]
    _assert(
        all_unique(pool_names),
        'Pool names are not unique:\n' + '\n'.join('  ' + x
                                                   for x in pool_names))
    _assert(all(s for s in pool_names), 'Pool name must not be empty')

    pool_types: Set[PoolT] = set()
    for pool in pools_desc:
        name = pool['name']
        t = pool_type(pool)
        if t is PoolT.sns:
            d = pool.get('allowed_failures')
            spare_units = pool.get('spare_units')
            _assert(
                spare_units is None or spare_units == 0
                or spare_units == pool['parity_units'],
                f'Pool {name}: spare_units must be either not set, or '
                "must be 0 or must be equal to 'parity_units'")
            if d:
                _assert(
                    d['disk'] <= pool['parity_units'],
                    "Pool {}: Cannot allow that many disk failures ({}). "
                    "The number of allowed disk failures must not exceed "
                    "the number of spare disks (parity_units={}).".format(
                        name, d['disk'], pool['parity_units']))
        else:
            data_units = pool['data_units']
            _assert(data_units == 1,
                    f"Pool {name}: Wrong number of data_units ({data_units}). "
                    f"Pools of {t!r} type can only have 1 data unit.")
            _assert(t not in pool_types,
                    f'Pool {name}: No more than one {t!r} pool can be defined')
        pool_types.add(t)

        if 'disk_refs' not in pool:
            continue

        # XXX Do we want to support disk_refs in MD pools as well?
        _assert(
            t in (PoolT.sns, PoolT.dix),
            f"Pool {name}: disk_refs are only supported for pools of either "
            "'sns' or 'dix' types")
        disk_refs = pool['disk_refs']
        # Allowing diskrefs to be empty until the CDF generation is fixed.
        if disk_refs:
            _assert(
                disk_refs, f'Pool {name}: disk_refs must not be empty'
                ' (it can be omitted though)')
            _assert(all_unique(repr(x) for x in disk_refs),
                    f'Pool {name}: disk_refs must be unique')

    _assert(PoolT.sns in pool_types,
            "At least one pool of 'sns' type must be defined")

    sns_pools = [pool for pool in pools_desc if pool_type(pool) is PoolT.sns]
    unrestricted = [pool['name'] for pool in sns_pools
                    if 'disk_refs' not in pool]
    _assert(len(unrestricted) <= 1,
            'These pools compete for disks: {}. Leave only one of them '
            'or allow disks using disk_refs'.format(
            ', '.join(unrestricted)))
    _assert(any('disk_refs' in pool for pool in sns_pools) ==
            (not unrestricted),
            "'sns' pools with and without disk_refs cannot be used together")


def validate_profiles_desc(profiles_desc: List[Dict[str, Any]],
                           pools_desc: List[Dict[str, Any]]) -> None:
    logging.debug('Validating profile descriptions')
    _assert(bool(profiles_desc),
            'List of profiles must not be empty (it can be omitted though)')
    _assert(
        all_unique(x['name'] for x in profiles_desc),
        'Profile names are not unique:\n' + '\n'.join('  ' + x['name']
                                                      for x in profiles_desc))
    _assert(all(x['name'] for x in profiles_desc),
            'Profile name must not be empty')

    Pool = NamedTuple('Pool', [('name', str), ('type', PoolT)])
    all_pools = [Pool(pool['name'], pool_type(pool)) for pool in pools_desc]

    for prof in profiles_desc:
        name, pools = prof['name'], prof['pools']
        _assert(pools, f"Profile {name!r} doesn't refer to any pools")
        for case, select in [('unknown', lambda _: True)]:
            bad = [
                s for s in pools if s not in
                [pool.name for pool in all_pools if select(pool.type)]
            ]
            _assert(
                not bad, 'Profile {!r} refers to {} pool{}: {}'.format(
                    name, case, 's' if len(bad) > 1 else '', ', '.join(bad)))
        _assert(all_unique(pools),
                f'Profile {name!r} has non-unique pool references')


def validate_fdmi_filters_desc(fdmi_filters_desc: List[Dict[str, Any]],
                               nodes_desc: List[Dict[str, Any]]) \
        -> None:
    logging.debug('Validating FDMI filters')
    _assert(
        all_unique(x['name'] for x in fdmi_filters_desc),
        'FDMI filter names are not unique:\n' +
        '\n'.join('  ' + x['name'] for x in fdmi_filters_desc))
    _assert(all(x['name'] for x in fdmi_filters_desc),
            'FDMI filter name must not be empty')

    Node = NamedTuple('Node', [('name', str), ('clients_nr', int)])
    nodes = [Node(name=x['hostname'], clients_nr=x['m0_clients']['other'])
             for x in nodes_desc]

    for fdmi_filter_desc in fdmi_filters_desc:
        node = fdmi_filter_desc['node']
        client_index = fdmi_filter_desc['client_index']
        _assert(node, f"FDMI filter {fdmi_filter_desc} "
                "doesn't refer to any node")
        matching_nodes = [n for n in nodes if n.name == node]
        _assert(
            len(matching_nodes) == 1,
            f"FDMI filter have exactly one node: {fdmi_filter_desc}")
        _assert(client_index < matching_nodes[0].clients_nr,
                "FDMI filter m0_clients index is out of bound: "
                f"client_index={client_index}, node={matching_nodes[0]}")


@lru_cache(maxsize=1)
def minion_id() -> Optional[str]:
    path = '/etc/salt/minion_id'
    if not os.path.isfile(path):
        return None
    with open(path) as f:
        return f.readline().strip()


def get_all_local_addrs() -> List[str]:
    interface_details = psutil.net_if_addrs()
    ips = []
    for iface in interface_details.values():
        ips.append(iface[0].address)
    return ips


def is_localhost(hostname: str) -> bool:
    assert hostname
    m_id = minion_id()
    ids = [m_id] if m_id is not None else []
    ids.extend(get_all_local_addrs())
    for item in socket.gethostbyname_ex(hostname):
        if any(x in item for x in ids):
            return True
    return False


@repeat_on_cmd_timeout(max_retries=-1)
def run_command(hostname: str, *args: str) -> str:
    assert hostname
    return subprocess.check_output(args if is_localhost(hostname) else
                                   ['ssh', hostname, *args],
                                   timeout=15).decode()


def validate_facter(hostname: str) -> None:
    ver_str = run_command(hostname, 'facter', '--version')
    # The output may contain commit id like this:
    # 3.14.8 (commit 4339472f441868ecdae694ffc71e7c8ed0fc24e3)
    ver_str = ver_str.split(' ')[0]

    if version(ver_str) >= version('3.14.2'):
        return
    raise RuntimeError(
        f'Unsupported facter version found at node {hostname}: {ver_str}. '
        'Please use 3.14.2 or higher.')


def get_facts(hostname: str, mock_p: bool, *args: str) -> Dict[str, Any]:
    if mock_p:
        logging.debug('Node facts will be fabricated because of --mock flag')
        return fabricate_facts(hostname, *args)
    validate_facter(hostname)
    result: Dict[str, Any] = json.loads(
        run_command(hostname, 'facter', '--json', *args))
    return result


def fabricate_facts(hostname: str, *args: str) -> Dict[str, Any]:
    rng = random.randrange
    ipaddress = '.'.join(str(n) for n in [
        random.choice([10, 172, 192]), rng(256), rng(256), rng(256)])
    fabricated = {
        'processorcount': rng(1, 21),
        'memorysize_mb': '{:.2f}'.format(random.uniform(512, 16000))
    }
    return dict((k, ipaddress if k.startswith('ipaddress_') else fabricated[k])
                for k in args)


Disk = NamedTuple('Disk', [('path', str), ('size', int), ('blksize', int)])
Disk.size.__doc__ = 'Total size, in bytes'
Disk.blksize.__doc__ = 'Block size for file system I/O'


# XXX see build_disk_info() in motr's `utils/m0genfacts`
def get_disks_ssh(hostname: str, mock_p: bool,
                  data_disks: List[Dict[str, Any]]) -> List[Disk]:
    if not data_disks:
        return []
    paths = []
    for d in data_disks:
        paths.append(d['path'])
    if mock_p:
        return fabricate_disks(hostname, paths)

    code = f"""\
import io
import os

# os.path.getsize() and os.stat().st_size don't work well with loop devices,
# they always return 0.
def blockdev_size(path):
    with open(path, 'rb') as f:
        return f.seek(0, io.SEEK_END)

print([dict(path=path,
            size=blockdev_size(path),
            blksize=os.stat(path).st_blksize)
       for path in {paths!r}])
"""
    if not is_localhost(hostname):
        code = f'"{code}"'

    return [Disk(**kwargs) for kwargs in
            ast.literal_eval(run_command(hostname,
                                         'sudo', 'python3', '-c', code))]


# XXX see build_disk_info() in motr's `utils/m0genfacts`
def get_disks_from_cdf(hostname: str, mock_p: bool,
                       data_disks: List[Dict[str, Any]]) -> List[Disk]:
    if not data_disks:
        return []

    disks = []
    for disk in data_disks:
        if not all(key in disk.keys() for key in ('path', 'size', 'blksize')):
            return []
        disks.append(Disk(path=disk['path'], size=disk['size'],
                          blksize=disk['blksize']))

    return disks


def fabricate_disks(hostname: str, paths: List[str]) -> List[Disk]:
    assert paths
    return [Disk(path=x, size=0, blksize=4096) for x in paths]


ObjT = Enum('ObjT', 'root fdmi_flt_grp fdmi_filter'
            ' node process service sdev'  # software subtree
            ' site rack enclosure controller drive'  # hardware subtree
            ' pool pver pver_f objv'  # pools subtree
            ' profile')
ObjT.__doc__ = 'Motr conf object type'


def objT_to_dhall(t: ObjT) -> str:
    return 'types.ObjT.' + ''.join(s.capitalize() for s in t.name.split('_'))


Oid = NamedTuple('Oid', [('type', ObjT), ('fidk', int)])
Oid.__doc__ = 'Motr conf object identifier'
Oid.fidk.__doc__ = '.f_key part of the corresponding m0_fid'


def oid_to_fidstr(oid: Oid) -> str:
    """Convert Oid to string representation of fid.

    >>> oid_to_fidstr(Oid(type=ObjT.process, fidk=85))
    '0x7200000000000001:0x55'
    """
    assert oid.type is not ObjT.pver_f
    fid_tcontainer = ord({
        ObjT.root: 't', ObjT.fdmi_flt_grp: 'g', ObjT.fdmi_filter: 'l',
        # software subtree
        ObjT.node: 'n', ObjT.process: 'r', ObjT.service: 's', ObjT.sdev: 'd',
        # hardware subtree
        ObjT.site: 'S', ObjT.rack: 'a', ObjT.enclosure: 'e',
        ObjT.controller: 'c', ObjT.drive: 'k',
        # pools subtree
        ObjT.pool: 'o', ObjT.pver: 'v', ObjT.objv: 'j', ObjT.profile: 'p'
    }[oid.type])
    return '{:#x}:{:#x}'.format(fid_tcontainer << 56 | 1, oid.fidk)


def _infinite_counter(start: int = 0) -> Iterator[int]:
    k = start
    while True:
        yield k
        k += 1


fidk_gen = _infinite_counter()  # fid key generator


def new_oid(objt: ObjT) -> Oid:
    return Oid(objt, next(fidk_gen))


def oid_to_dhall(oid: Oid) -> str:
    return f'utils.zoid {objT_to_dhall(oid.type)} {oid.fidk}'


def oids_to_dhall(oids: List[Oid]) -> str:
    if not oids:
        return '[] : List types.Oid'
    return '[ {} ]'.format(', '.join(oid_to_dhall(x) for x in oids))


NetProtocol = Enum('NetProtocol', 'tcp o2ib')


ProcT = Enum('ProcT', 'hax m0_server m0_client_s3 m0_client_other')
ProcT.__doc__ = 'Type of process'


class PortalGroup(IntEnum):
    hax = 22000,
    m0_server = 21000,
    m0_client_s3 = 22500,
    m0_client_other = 21500


class Portals:
    def __init__(self, hax=22000, m0_server=21000, m0_client_s3=22500,
                 m0_client_other=21500):
        self.hax = hax
        self.m0_server = m0_server
        self.m0_client_s3 = m0_client_s3
        self.m0_client_other = m0_client_other

    def get_portal_group(self):
        return IntEnum('PortalGroup',
                       {'hax': self.hax,
                        'm0_server': self.m0_server,
                        'm0_client_s3': self.m0_client_s3,
                        'm0_client_other': self.m0_client_other})


def get_lnet_portal_group(hax=1, m0_server=2,
                          m0_client_s3=3, m0_client_other=4) -> Portals:
    return Portals(hax=hax, m0_server=m0_server, m0_client_s3=m0_client_s3,
                   m0_client_other=m0_client_other)


def get_libfab_portal_group(hax=22000, m0_server=21000,
                            m0_client_s3=22500,
                            m0_client_other=21500) -> Portals:
    return Portals(hax=hax, m0_server=m0_server, m0_client_s3=m0_client_s3,
                   m0_client_other=m0_client_other)


class Endpoint:
    pass


class LnetEndpoint(Endpoint):
    _tmids: Dict[Tuple[str, int], int] = {}

    def __init__(self, proto: NetProtocol, ipaddr: str,
                 portalgroup: PortalGroup,
                 tmid: int = None):
        self.proto = proto
        assert ipaddr
        self.ipaddr = ipaddr
        assert portalgroup.value > 0
        self.portalgroup = portalgroup
        if tmid is None:
            self.tmid = self._tmids.setdefault((ipaddr, portalgroup.value), 1)
            self._tmids[(ipaddr, portalgroup.value)] += 1
        else:
            assert tmid > 0
            self.tmid = tmid

    def __repr__(self):
        args = ', '.join([f'proto={self.proto!r}',
                          f'ipaddr={self.ipaddr!r}',
                          f'portal={self.portalgroup}',
                          f'tmid={self.tmid}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self):
        return f'utils.lnetendpoint types.Protocol.{self.proto.name}' \
            f' "{self.ipaddr}" {self.portalgroup} {self.tmid}'

    def __str__(self):
        ip = self.ipaddr
        proto = self.proto.name
        return f'{ip}@{proto}:12345:{self.portalgroup}:{self.tmid}'


class LibfabricEndpoint(Endpoint):
    ep_map: Dict[Tuple[str, int], int] = {}

    def __init__(self, NetFamily: str, proto: NetProtocol, ipaddr: str,
                 portalgroup: PortalGroup):
        self.netfamily = NetFamily
        self.proto = proto
        assert ipaddr
        self.ipaddr = ipaddr
        assert portalgroup.value > 0
        self.portalgroup = portalgroup
        self.portal = 0
        self.portal = self.ep_map.setdefault((ipaddr, portalgroup.value),
                                             (portalgroup.value + 1))
        self.ep_map[(ipaddr, portalgroup.value)] += 1

    def __repr__(self):
        args = ', '.join([f'netfamily={self.netfamily!r}',
                          f'proto={self.proto!r}',
                          f'ipaddr={self.ipaddr!r}',
                          f'portal={self.portal}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self):
        return f'utils.libfabricendpoint types.NetFamily.{self.netfamily} ' \
            f'types.Protocol.{self.proto.name}' \
            f' "{self.ipaddr}" {self.portal}'

    def __str__(self):
        netfamily = self.netfamily
        ip = self.ipaddr
        proto = self.proto.name
        return f'{netfamily}:{proto}:{ip}@{self.portal}'


class ToDhall(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def to_dhall(self, oid: Oid) -> str:
        pass


Downlink = NamedTuple('Downlink', [('name', str), ('children_t', ObjT)])
Downlink.__doc__ = 'parent -> children relation'


class ConfRoot(ToDhall):
    _objt = ObjT.root
    _downlinks = {Downlink('nodes', ObjT.node),
                  Downlink('sites', ObjT.site),
                  Downlink('pools', ObjT.pool),
                  Downlink('profiles', ObjT.profile),
                  Downlink('fdmi_flt_grps', ObjT.fdmi_flt_grp)}

    def __init__(self, nodes: List[Oid], sites: List[Oid], pools: List[Oid],
                 profiles: List[Oid],
                 fdmi_flt_grps: List[Oid], mdpool: Oid = None,
                 mdredundancy: int = 0, imeta_pver: Oid = None):
        assert all(x.type is ObjT.node for x in nodes)
        assert all(x.type is ObjT.site for x in sites)
        assert all(x.type is ObjT.pool for x in pools)
        assert all(x.type is ObjT.profile for x in profiles)
        assert all(x.type is ObjT.fdmi_flt_grp for x in fdmi_flt_grps)
        assert mdpool is None or mdpool.type is ObjT.pool
        assert imeta_pver is None or imeta_pver.type is ObjT.pver

        self.nodes = nodes
        self.sites = sites
        self.pools = pools
        self.profiles = profiles
        self.fdmi_flt_grps = fdmi_flt_grps
        self.mdpool = mdpool
        self.mdredundancy = mdredundancy
        self.imeta_pver = imeta_pver

    def __repr__(self):
        args = ', '.join([f'nodes={self.nodes}',
                          f'sites={self.sites}',
                          f'pools={self.pools}',
                          f'profiles={self.profiles}',
                          f'fdmi_flt_grps={self.fdmi_flt_grps}',
                          f'mdpool={self.mdpool}',
                          f'mdredundancy={self.mdredundancy}',
                          f'imeta_pver={self.imeta_pver}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.root
        assert self.imeta_pver is not None
        assert self.mdpool is not None
        args = ', '.join([
            f'id = {oid_to_dhall(oid)}',
            f'mdpool = {oid_to_dhall(self.mdpool)}',
            f'mdredundancy = {self.mdredundancy}',
            f'imeta_pver = Some ({oid_to_dhall(self.imeta_pver)})',
            f'nodes = {oids_to_dhall(self.nodes)}',
            f'sites = {oids_to_dhall(self.sites)}',
            f'pools = {oids_to_dhall(self.pools)}',
            f'profiles = {oids_to_dhall(self.profiles)}',
            f'fdmi_flt_grps = {oids_to_dhall(self.fdmi_flt_grps)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any]) -> Oid:
        root_id = new_oid(ObjT.root)
        m0conf[root_id] = cls(nodes=[], sites=[], pools=[], profiles=[],
                              fdmi_flt_grps=[])
        return root_id


class ConfNode(ToDhall):
    _objt = ObjT.node
    _downlinks = {Downlink('processes', ObjT.process)}

    def __init__(self, name: str, nr_cpu: int, memsize_MB: int,
                 processes: List[Oid], machine_id: Optional[str] = None):
        self.name = name
        self.nr_cpu = nr_cpu
        self.memsize_MB = memsize_MB
        self.machine_id = machine_id

        _assert(all(x.type is ObjT.process for x in processes),
                'All processes must be of ObjT.process type')
        self.processes = processes

    def __repr__(self):
        args = ', '.join([f'nr_cpu={self.nr_cpu}',
                          f'memsize_MB={self.memsize_MB}',
                          f'processes={self.processes}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        _assert(oid.type is ObjT.node, 'The argument must be ObjT.node but '
                f'{oid.type} was given.')
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'nr_cpu = {self.nr_cpu}',
                          f'memsize_MB = {self.memsize_MB}',
                          f'processes = {oids_to_dhall(self.processes)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              node_desc: Dict[str, Any]) -> Oid:
        _assert(parent.type is ObjT.root, 'Parent type must be ObjT.root')
        node_id = new_oid(ObjT.node)
        facts = node_desc['facts']
        m0conf[node_id] = cls(name=node_desc['hostname'],
                              nr_cpu=facts['processorcount'],
                              memsize_MB=facts['_memsize_MB'],
                              processes=[],
                              machine_id=node_desc.get('machine_id'))
        m0conf[parent].nodes.append(node_id)
        return node_id


class ConfProcess(ToDhall):
    _objt = ObjT.process
    _downlinks = {Downlink('services', ObjT.service)}

    def __init__(self, nr_cpu: int, memsize_MB: int,
                 endpoint,
                 services: List[Oid], meta_data: str = None):
        _assert(nr_cpu > 0, 'nr_cpu must be positive')
        self.nr_cpu = nr_cpu
        _assert(memsize_MB > 0, 'memsize_MB must be positive')
        self.memsize_MB = memsize_MB
        self.endpoint = endpoint
        self.meta_data = meta_data
        _assert(all(x.type is ObjT.service for x in services),
                'All services must be of ObjT.service type')
        self.services = services

    def __repr__(self):
        args = ', '.join([f'nr_cpu={self.nr_cpu}',
                          f'memsize_MB={self.memsize_MB}',
                          f'endpoint={self.endpoint!r}',
                          f'services={self.services}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        _assert(oid.type is ObjT.process,
                f'ObjT.process expected, but {oid.type} was given')
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'nr_cpu = {self.nr_cpu}',
                          f'memsize_MB = {self.memsize_MB}',
                          f'endpoint = "{self.endpoint}"',
                          f'services = {oids_to_dhall(self.services)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls,
              m0conf: Dict[Oid, Any],
              parent: Oid,
              portalgroup: Portals,
              node_desc: Dict[str, Any],
              proc_t: ProcT,
              proc_desc: Dict[str, Any] = None,
              ctrl: Optional[Oid] = None) -> Oid:
        _assert(parent.type is ObjT.node,
                f'Parent must be ObjT.node but {parent.type} was given')
        if ctrl is not None:
            _assert(
                ctrl.type is ObjT.controller, 'ctrl must be of type '
                f'Obj.controller, but {ctrl.type} was given')

        facts = node_desc['facts']
        proto = node_desc.get('data_iface_type', 'tcp')
        transport_type = node_desc.get('transport_type', 'libfab')
        portal_group = portalgroup.get_portal_group()
        ep: Any = (
            LibfabricEndpoint(NetFamily='inet', proto=NetProtocol[proto],
                              ipaddr=facts[ipaddr_key(
                                           node_desc['data_iface'])],
                              portalgroup=portal_group[proc_t.name]))
        if transport_type == 'lnet':
            # Creates Lnet endpoint, remove once deprecated.
            ep = LnetEndpoint(proto=NetProtocol[proto],
                              ipaddr=facts[ipaddr_key(
                                           node_desc['data_iface'])],
                              portalgroup=portal_group[proc_t.name])
        proc_id = new_oid(ObjT.process)
        meta_data = None
        if proc_desc is not None and proc_t is ProcT.m0_server:
            meta_data = proc_desc['io_disks'].get('meta_data')

        m0conf[proc_id] = cls(nr_cpu=facts['processorcount'],
                              memsize_MB=facts['_memsize_MB'],
                              endpoint=ep,
                              meta_data=meta_data,
                              services=[])
        m0conf[parent].processes.append(proc_id)

        for stype in service_types(proc_t, proc_desc):
            svc_id = ConfService.build(m0conf, proc_id, stype, ep, ctrl)
            if stype is SvcT.M0_CST_IOS:
                assert proc_desc is not None
                for disk in proc_desc['_io_disks']:
                    assert ctrl is not None
                    ConfDrive.build(m0conf, ctrl,
                                    ConfSdev.build(m0conf, svc_id, disk))
        return proc_id


def build_cas_devs(m0conf: Dict[Oid, Any],
                   ctrls: Dict[Oid, Any]):
    for item in list(m0conf):
        if item.type is ObjT.enclosure:
            encl_id = item
            for proc_id in m0conf[m0conf[encl_id].node].processes:
                for svc_id in m0conf[proc_id].services:
                    if m0conf[svc_id].type is SvcT.M0_CST_CAS:
                        ctrl_id = m0conf[svc_id].ctrl_id
                        proc_desc = ctrls[ctrl_id]
                        meta_data = proc_desc['io_disks'].get('meta_data')
                        if not meta_data or meta_data is None:
                            meta_data = '/dev/null'
                        ConfDrive.build(m0conf, ctrl_id,
                                        ConfSdev.build(m0conf, svc_id,
                                                       Disk(path=meta_data,
                                                            size=1024,
                                                            blksize=1)))


# m0_conf_service_type
class SvcT(Enum):
    """Motr service type
    """
    M0_CST_MDS = 1
    M0_CST_IOS = auto()
    M0_CST_CONFD = auto()
    M0_CST_RMS = auto()
    M0_CST_STATS = auto()
    M0_CST_HA = auto()
    M0_CST_SSS = auto()
    M0_CST_SNS_REP = auto()
    M0_CST_SNS_REB = auto()
    M0_CST_ADDB2 = auto()
    M0_CST_CAS = auto()
    M0_CST_DIX_REP = auto()
    M0_CST_DIX_REB = auto()
    M0_CST_DS1 = auto()
    M0_CST_DS2 = auto()
    M0_CST_FIS = auto()
    M0_CST_FDMI = auto()
    M0_CST_BE = auto()
    M0_CST_M0T1FS = auto()
    M0_CST_CLIENT = auto()
    M0_CST_ISCS = auto()

    def to_dhall(self) -> str:
        return f'types.{self}'


def service_types(proc_t: ProcT,
                  proc_desc: Dict[str, Any] = None) -> List[SvcT]:
    ts = []
    if proc_t is ProcT.hax:
        ts.append(SvcT.M0_CST_HA)
    ts.append(SvcT.M0_CST_RMS)
    if proc_t is ProcT.m0_server:
        assert proc_desc is not None
        if proc_desc.get('runs_confd'):
            ts.append(SvcT.M0_CST_CONFD)
        if proc_desc['_io_disks']:
            ts.extend([SvcT.M0_CST_IOS,
                       SvcT.M0_CST_SNS_REP,
                       SvcT.M0_CST_SNS_REB,
                       SvcT.M0_CST_ADDB2,
                       SvcT.M0_CST_CAS,
                       SvcT.M0_CST_ISCS,
                       SvcT.M0_CST_FDMI])
    return ts


class ConfService(ToDhall):
    _objt = ObjT.service
    _downlinks = {Downlink('sdevs', ObjT.sdev)}

    def __init__(self, stype: SvcT, endpoint,
                 sdevs: List[Oid], ctrl_id: Oid = None):
        self.type = stype
        self.endpoint = endpoint
        assert all(x.type is ObjT.sdev for x in sdevs)
        self.sdevs = sdevs
        self.ctrl_id = ctrl_id

    def __repr__(self):
        args = ', '.join([f'stype={self.type}',
                          f'endpoint={self.endpoint!r}',
                          f'sdevs={self.sdevs}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.service
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'type = {self.type.to_dhall()}',
                          f'endpoint = "{self.endpoint}"',
                          f'sdevs = {oids_to_dhall(self.sdevs)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              stype: SvcT, endpoint,
              ctrl: Optional[Oid] = None) -> Oid:
        assert parent.type is ObjT.process
        svc_id = new_oid(ObjT.service)
        m0conf[svc_id] = cls(stype, endpoint, sdevs=[], ctrl_id=ctrl)
        m0conf[parent].services.append(svc_id)
        return svc_id


class ConfSdev(ToDhall):
    _dev_idx = _infinite_counter()
    _objt = ObjT.sdev
    _downlinks: Set[Downlink] = set()

    def __init__(self, dev_idx: int, filename: str, size: int, blksize: int):
        assert dev_idx >= 0
        assert filename
        assert size >= 0
        assert blksize >= 0

        self.dev_idx = dev_idx
        self.filename = filename
        self.size = size
        self.blksize = blksize

    def __repr__(self):
        args = ', '.join([f'dev_idx={self.dev_idx}',
                          f'filename={self.filename!r}',
                          f'size={self.size}',
                          f'blksize={self.blksize}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.sdev
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'dev_idx = {self.dev_idx}',
                          f'filename = "{self.filename}"',
                          f'size = {self.size}',
                          f'blksize = {self.blksize}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              disk_desc: Disk) -> Oid:
        assert parent.type is ObjT.service
        sdev_id = new_oid(ObjT.sdev)
        m0conf[sdev_id] = cls(dev_idx=next(cls._dev_idx),
                              filename=disk_desc.path,
                              size=disk_desc.size,
                              blksize=disk_desc.blksize)
        m0conf[parent].sdevs.append(sdev_id)
        return sdev_id


class ConfSite(ToDhall):
    _objt = ObjT.site
    _downlinks = {Downlink('racks', ObjT.rack)}

    def __init__(self, racks: List[Oid], pvers: List[Oid]):
        assert all(x.type is ObjT.rack for x in racks)
        self.racks = racks
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'racks={self.racks}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.site
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'racks = {oids_to_dhall(self.racks)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid) -> Oid:
        assert parent.type is ObjT.root
        site_id = new_oid(ObjT.site)
        m0conf[site_id] = cls(racks=[], pvers=[])
        assert not m0conf[parent].sites
        m0conf[parent].sites = [site_id]  # NB: a single site
        return site_id


class ConfRack(ToDhall):
    _objt = ObjT.rack
    _downlinks = {Downlink('encls', ObjT.enclosure)}

    def __init__(self, encls: List[Oid], pvers: List[Oid]):
        assert all(x.type is ObjT.enclosure for x in encls)
        self.encls = encls
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'encls={self.encls}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.rack
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'encls = {oids_to_dhall(self.encls)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid) -> Oid:
        assert parent.type is ObjT.site
        rack_id = new_oid(ObjT.rack)
        m0conf[rack_id] = cls(encls=[], pvers=[])
        m0conf[rack_id]._parent = parent
        assert not m0conf[parent].racks
        m0conf[parent].racks = [rack_id]  # NB: a single rack
        return rack_id


class ConfEnclosure(ToDhall):
    _objt = ObjT.enclosure
    _downlinks = {Downlink('ctrls', ObjT.controller)}

    def __init__(self, node: Oid, ctrls: List[Oid], pvers: List[Oid]):
        assert node.type is ObjT.node
        self.node = node
        assert all(x.type is ObjT.controller for x in ctrls)
        self.ctrls = ctrls
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'node={self.node}',
                          f'ctrls={self.ctrls}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.enclosure
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'node = {oid_to_dhall(self.node)}',
                          f'ctrls = {oids_to_dhall(self.ctrls)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid, node: Oid) -> Oid:
        assert parent.type is ObjT.rack
        assert node.type is ObjT.node

        encl_id = new_oid(ObjT.enclosure)
        m0conf[encl_id] = cls(node, ctrls=[], pvers=[])
        m0conf[encl_id]._parent = parent
        m0conf[parent].encls.append(encl_id)
        return encl_id


class ConfController(ToDhall):
    _objt = ObjT.controller
    _downlinks = {Downlink('drives', ObjT.drive)}

    def __init__(self, drives: List[Oid], pvers: List[Oid]):
        assert all(x.type is ObjT.drive for x in drives)
        self.drives = drives
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'drives={self.drives}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.controller
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'drives = {oids_to_dhall(self.drives)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid) -> Oid:
        assert parent.type is ObjT.enclosure

        ctrl_id = new_oid(ObjT.controller)
        m0conf[ctrl_id] = cls(drives=[], pvers=[])
        m0conf[ctrl_id]._parent = parent
        m0conf[parent].ctrls.append(ctrl_id)
        return ctrl_id


class ConfDrive(ToDhall):
    _objt = ObjT.drive
    _downlinks: Set[Downlink] = set()

    def __init__(self, sdev: Oid, pvers: List[Oid]):
        assert sdev.type is ObjT.sdev
        self.sdev = sdev
        assert all(x.type is ObjT.pver for x in pvers)
        self.pvers = pvers

    def __repr__(self):
        args = ', '.join([f'sdev={self.sdev}',
                          f'pvers={self.pvers}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.drive
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'sdev = {oid_to_dhall(self.sdev)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid, sdev: Oid) -> Oid:
        assert parent.type is ObjT.controller
        assert sdev.type is ObjT.sdev

        drive_id = new_oid(ObjT.drive)
        m0conf[drive_id] = cls(sdev, pvers=[])
        m0conf[drive_id]._parent = parent
        m0conf[parent].drives.append(drive_id)
        return drive_id


PoolT = Enum('PoolT', 'sns dix md')  # PoolType.dhall


def pool_type(pool_desc: Dict[str, Any]) -> PoolT:
    return PoolT[pool_desc.get('type', 'sns')]


def cluster_svcs(m0conf: Dict[Oid, Any], svc_t) -> List[Oid]:
    return [svc_id
            for encl_id in m0conf if encl_id.type is ObjT.enclosure
            for proc_id in m0conf[m0conf[encl_id].node].processes
            for svc_id in m0conf[proc_id].services
            if m0conf[svc_id].type is svc_t]


# node-to-drives mapping
def pool_drives_with_nodes(m0conf: Dict[Oid, Any]) -> List[Tuple[Oid, Oid]]:
    all_drives: List[Tuple[Oid, Oid]] = [
        (drive_id, m0conf[encl_id].node)
        for encl_id in m0conf if encl_id.type is ObjT.enclosure
        for ctrl_id in m0conf[encl_id].ctrls
        for drive_id in m0conf[ctrl_id].drives
    ]
    assert all_drives
    assert all(drive_id.type is ObjT.drive and node_id.type is ObjT.node
               for drive_id, node_id in all_drives)

    return all_drives


def pool_drives_from_references(
                m0conf: Dict[Oid, Any],
                pool_desc: Dict[str, Any]) -> List[Tuple[Oid, Oid]]:
    drives = []
    all_drives: List[Tuple[Oid, Oid]] = pool_drives_with_nodes(m0conf)
    disk_refs = get_pool_diskrefs(pool_desc)

    unique_list_of_tuples = list(set(all_drives))

    if disk_refs is not None:
        for ref in disk_refs:
            targets = [(drive_id, node_id)
                       for drive_id, node_id in unique_list_of_tuples
                       if ref['path'] == m0conf[m0conf[drive_id].sdev].filename
                       and ref.get('node') in (None, m0conf[node_id].name)]
            # XXX Improve validate_pools_desc() to catch this error.
            assert len(targets) == 1, 'Check {} config line'.format(ref)
            drives.extend(targets)
        assert all_unique(drives), \
            'Pool {!r}: some of disk_refs refer to the same disk'.format(
                pool_desc['name'])
    return drives


# The job of this function is to return list of drives.
def pool_drives(m0conf: Dict[Oid, Any],
                pool_desc: Dict[str, Any]) -> List[Oid]:
    ptype = pool_type(pool_desc)

    if ptype is PoolT.dix:
        return [m0conf[ctrl_id].drives[-1]
                for ctrl_id in m0conf if ctrl_id.type is ObjT.controller]

    if ptype is PoolT.md:
        return [m0conf[ctrl_id].drives[0]
                for ctrl_id in m0conf if ctrl_id.type is ObjT.controller]

    assert ptype is PoolT.sns
    drives = pool_drives_from_references(m0conf, pool_desc)
    if not drives:
        drives = pool_drives_with_nodes(m0conf)

    return [drive_id for drive_id, _ in drives]


# we don't allow overlapping pools, i.e. pools sharing physical devices
# between them. So, this function validate list of drives for a give pools
# and It disables disk repeation in base pools.
def check_drive_multiple_references(
                m0conf: Dict[Oid, Any],
                cluster_desc: Dict[str, Any]) -> List[Tuple[Oid, Oid]]:
    drives = []
    for pool in cluster_desc['pools']:
        drives.extend(pool_drives_from_references(m0conf, pool))
    over_referenced: Dict[Oid, List[Oid]] = {}  # node-to-drives mapping

    for drive_ref in drives:
        if drives.count(drive_ref) > 1:
            over_referenced.setdefault(drive_ref[1], []).append(drive_ref[0])

    create_aux_val = cluster_desc.get('create_aux')
    if not create_aux_val:
        if over_referenced:
            err = '{} referred to by several pools:'.format(
                'This disk is' if len(over_referenced) == 1 else
                'These disks are')
            for node_id in sorted(over_referenced):
                err += '\n  ' + m0conf[node_id].name
                for drive_id in sorted(over_referenced[node_id]):
                    err += '\n   \\_ ' + m0conf[m0conf[drive_id].sdev].filename
            raise AssertionError(err)
    return drives


def cluster_encls(m0conf: Dict[Oid, Any], svc_t) -> List[Oid]:
    return list({encl_id
                 for encl_id in m0conf if encl_id.type is ObjT.enclosure
                 for proc_id in m0conf[m0conf[encl_id].node].processes
                 for svc_id in m0conf[proc_id].services
                 if m0conf[svc_id].type is svc_t})


def cluster_ctrls(m0conf: Dict[Oid, Any]) -> List[Oid]:
    return [m0conf[encl_id].ctrls
            for encl_id in cluster_encls(m0conf, SvcT.M0_CST_IOS)]


def drives_per_node_and_ctrl(m0conf: Dict[Oid, Any]) -> faultToleranceInfo:
    max_drives_per_node = 0
    max_drives_per_ctrl = 0
    min_drives_per_node = 0
    drives_per_node = 0
    for encl_id in m0conf:
        if encl_id.type is ObjT.enclosure:
            for ctrl_id in m0conf[encl_id].ctrls:
                drives_per_ctrl = len(m0conf[ctrl_id].drives)
                drives_per_node += drives_per_ctrl
                if max_drives_per_ctrl < drives_per_ctrl:
                    max_drives_per_ctrl = drives_per_ctrl
            if max_drives_per_node < drives_per_node:
                max_drives_per_node = drives_per_node
            if (min_drives_per_node == 0 or
                    min_drives_per_node > drives_per_node):
                min_drives_per_node = drives_per_node
            drives_per_node = 0
    return faultToleranceInfo(max_drives_per_node=max_drives_per_node,
                              max_drives_per_ctrl=max_drives_per_ctrl,
                              min_drives_per_node=min_drives_per_node)


Failures = NamedTuple('Failures', [('site', int),
                                   ('rack', int),
                                   ('enclosure', int),
                                   ('controller', int),
                                   ('drive', int)])
Failures.__doc__ = """\
Failure tolerance vector.

For a given pool version, the failure tolerance vector reflects how
many devices in each level can be expected to fail whilst still
allowing the remaining disks in that pool version to be read.

For disks, then, note that this will equal the parameter K, where
(N,K,P) is the triple of data units, parity units, pool width for
the pool version.

For controllers, this should indicate the maximum number such that
no failure of that number of controllers can take out more than K
units.  We can put an upper bound on this by considering
floor((nr_encls)/(N+K)), though distributional issues may result
in a lower value.
"""
pver_levels = Failures._fields


class ConfPool(ToDhall):
    _objt = ObjT.pool
    _downlinks = {Downlink('pvers', ObjT.pver)}

    def __init__(self, pvers: List[Oid], name: Optional[str]):
        assert all(x.type in (ObjT.pver, ObjT.pver_f) for x in pvers)
        self.pvers = pvers
        self._name = name

    def __repr__(self):
        args = ', '.join([f'pvers={self.pvers}',
                          f'name={self._name!r}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.pool
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'pvers = {oids_to_dhall(self.pvers)}'])
        return '{ %s }' % args

    @classmethod
    def gen_sns_tolerance(cls, m0conf: Dict[Oid, Any],
                          pool_desc: Dict[str, Any],
                          cluster_desc: Dict[str, Any]) -> Failures:

        encls_nr = len(cluster_encls(m0conf, SvcT.M0_CST_IOS))
        ctrls_per_encl = floor(len(cluster_ctrls(m0conf)) / encls_nr)
        data_units = pool_desc['data_units']
        parity_units = pool_desc['parity_units']
        spare_units = pool_desc.get('spare_units', 0)
        total_units = data_units + parity_units + spare_units
        fault_tol_info = drives_per_node_and_ctrl(m0conf)
        min_pool_width = fault_tol_info.min_drives_per_node * encls_nr
        actual_pool_width = len(pool_drives(m0conf, pool_desc))
        if ((min_pool_width < actual_pool_width) and
                (min_pool_width / total_units) > 0):
            # Pool is asymmetric, all the nodes don't have same number of
            # drives.
            # if (min_pool_width / total_units) > 0 means that one of the
            # nodes can probably get more than K units of a parity group.
            # Thus we cannot allow node failures.
            enc_failure_allowed = 0
            if fault_tol_info.max_drives_per_ctrl > parity_units:
                ctrl_failure_allowed = 0
        else:
            enc_failure_allowed_FP = parity_units / ceil(total_units /
                                                         encls_nr)
            enc_failure_allowed = floor(enc_failure_allowed_FP)
            temp = ctrls_per_encl * enc_failure_allowed
            ctrl_failure_allowed = min(temp, parity_units)
        return Failures(0, 0, enc_failure_allowed, ctrl_failure_allowed,
                        parity_units)

    @classmethod
    def gen_md_tolerance(cls, m0conf: Dict[Oid, Any],
                         pool_desc: Dict[str, Any]) -> Failures:
        encls_nr = len(cluster_encls(m0conf, SvcT.M0_CST_IOS))
        ctrls_per_encl = floor(len(cluster_ctrls(m0conf)) / encls_nr)
        encl_failures = encls_nr - 1
        ctrl_failures = min(encl_failures, (ctrls_per_encl * encl_failures))
        return Failures(0, 0, encl_failures, ctrl_failures, encl_failures)

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              pool_desc: Dict[str, Any],
              cluster_desc: Dict[str, Any]) -> Oid:
        assert parent.type is ObjT.root

        pool_id = new_oid(ObjT.pool)
        m0conf[pool_id] = cls(pvers=[], name=pool_desc.get('name'))
        m0conf[parent].pools.append(pool_id)

        spare_units = pool_desc.get('spare_units')
        assert spare_units is None\
            or spare_units == 0\
            or spare_units == pool_desc['parity_units']

        if spare_units is None:
            spare_units = pool_desc.get('parity_units')

        drives = pool_drives(m0conf, pool_desc)
        t = pool_type(pool_desc)
        assert ('data_units' in pool_desc) == ('parity_units' in pool_desc)
        if 'data_units' in pool_desc:
            if spare_units is None:
                spare_units = pool_desc['parity_units']
            pd_attrs = PDClustAttrs0(pool_desc['data_units'],
                                     pool_desc['parity_units'],
                                     spare_units)
        else:
            if t is PoolT.dix:
                # For CAS service N must be equal to 1.  CAS records are
                # indivisible pieces of data, the whole CAS record is
                # always stored on one node.
                data_units = 1
                parity_units = len(cluster_encls(m0conf, SvcT.M0_CST_CAS)) - 1
            else:
                assert t is PoolT.md
                data_units = 1
                parity_units = len(cluster_encls(m0conf, SvcT.M0_CST_IOS)) - 1
            pd_attrs = PDClustAttrs0(data_units=data_units,
                                     parity_units=parity_units,
                                     spare_units=0)
        d = pool_desc.get('allowed_failures')
        if t in (PoolT.sns, PoolT.dix):
            if d:
                tolerance = Failures(d['site'], d['rack'], d['encl'],
                                     d['ctrl'], d['disk'])
            else:
                if t == PoolT.dix:
                    tolerance = ConfPool.gen_md_tolerance(m0conf, pool_desc)
                else:
                    tolerance = ConfPool.gen_sns_tolerance(m0conf, pool_desc,
                                                           cluster_desc)
        else:
            tolerance = ConfPool.gen_md_tolerance(m0conf, pool_desc)

        pver = ConfPver.build(m0conf, pool_id, pd_attrs, drives, tolerance)
        if t is PoolT.dix:
            assert m0conf[parent].imeta_pver is None
            m0conf[parent].imeta_pver = pver
        elif t is PoolT.md:
            assert m0conf[parent].mdpool is None
            m0conf[parent].mdpool = pool_id
        else:
            assert t is PoolT.sns
            for v in gen_allowances(tolerance):
                ConfPverF.build(m0conf, pool_id, base=pver, allowance=v)

        return pool_id


def gen_allowances(allowed: Failures) -> List[Failures]:
    """Generate all combinations of failures from CDF's `allowed_failures`
    vector.

    >>> gen_allowances(Failures(0, 0, 0, 0, 2))
    [Failures(0, 0, 0, 0, 1), Failures(0, 0, 0, 0, 2)]
    >>> gen_allowances(Failures(0, 0, 0, 1, 2))
    [Failures(0, 0, 0, 0, 1),
     Failures(0, 0, 0, 0, 2),
     Failures(0, 0, 0, 1, 0),
     Failures(0, 0, 0, 1, 1),
     Failures(0, 0, 0, 1, 2)]
    >>> gen_allowances(Failures(0, 0, 0, 0, 0))
    []
    """
    return [Failures(*v) for v in
            filter(any, itertools.product(*(range(n+1) for n in allowed)))]


# m0_pdclust_attr
PDClustAttrs0 = NamedTuple('PDClustAttrs0',
                           # XXX What about `unit_size` and `seed`?
                           [('data_units', int), ('parity_units', int),\
                            ('spare_units', int)])
PDClustAttrs0.__doc__ = 'Motr parity de-clustering layout attributes'


def tolerated_failures(m0conf: Dict[Oid, Any], pd_attrs: PDClustAttrs0,
                       drives: List[Oid]) -> Failures:
    assert pd_attrs.data_units > 0
    assert pd_attrs.parity_units >= 0
    assert pd_attrs.spare_units is None\
        or pd_attrs.spare_units == 0\
        or pd_attrs.spare_units == pd_attrs.parity_units
    assert drives and all(x.type is ObjT.drive for x in drives)
    assert all_unique(drives)

    n, k, s = pd_attrs

    nr_ctrls = 0
    for ctrl_id, ctrl in m0conf.items():
        if ctrl_id.type is not ObjT.controller:
            continue
        if not set(m0conf[ctrl_id].drives).isdisjoint(set(drives)):
            nr_ctrls += 1
    assert nr_ctrls > 0

    q, r = divmod(n + k + s, nr_ctrls)
    # There are `r` controllers with `q+1` units and
    # `(nr_ctrls - r)` controllers with `q` units.
    #
    # In the worst scenario the most populated controllers
    # will go down first.
    #
    # `kc` is the number of parity group units that will be
    # unavailable when those `r` controllers go down.
    kc = r * (q + 1)
    if kc > k:
        # We won't be able to recover data if `kc` units are lost.
        # Recalculate the tolerable number of controller failures
        # by distributing the tolerable number of unit failures
        # (`k`) among the "most populous" controllers (`q+1` units
        # in each).
        ctrl_failures = k // (q + 1)
    elif kc < k:
        # `kc` units (`r` controllers) are lost, but we can
        # tolerate losing `k - kc` more units.
        # `(k - kc) // q` is the number of additional controller
        # failures that we can tolerate.
        ctrl_failures = r + (k - kc) // q
    else:
        # kc == k.  We can lose precisely `r` controllers;
        # no more, no less.
        ctrl_failures = r
    return Failures(site=0, rack=0, enclosure=0, controller=ctrl_failures,
                    drive=k)


class ConfPver(ToDhall):
    _objt = ObjT.pver
    _downlinks = {Downlink('sitevs', ObjT.objv)}

    def __init__(self, data_units: int, parity_units: int, spare_units: int,
                 pool_width: int, tolerance: List[int], sitevs: List[Oid]):
        assert data_units > 0
        assert parity_units >= 0
        assert spare_units is None\
            or spare_units == 0\
            or spare_units == parity_units
        assert pool_width > 0
        assert len(tolerance) == len(pver_levels)
        assert all(n >= 0 for n in tolerance)
        assert all(x.type is ObjT.objv for x in sitevs)

        self.data_units = data_units
        self.parity_units = parity_units
        self.spare_units = spare_units
        self.pool_width = pool_width
        self.tolerance = tolerance
        self.sitevs = sitevs

    def __repr__(self):
        args = ', '.join([f'data_units={self.data_units}',
                          f'parity_units={self.parity_units}',
                          f'spare_units={self.spare_units}',
                          f'pool_width={self.pool_width}',
                          f'tolerance={self.tolerance}',
                          f'sitevs={self.sitevs}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.pver
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'data_units = {self.data_units}',
                          f'parity_units = {self.parity_units}',
                          f'spare_units = {self.spare_units}',
                          f'pool_width = {self.pool_width}',
                          f'tolerance = {self.tolerance}',
                          f'sitevs = {oids_to_dhall(self.sitevs)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              pd_attrs: PDClustAttrs0, drives: List[Oid],
              tolerance: Failures = None) -> Oid:
        assert parent.type is ObjT.pool
        assert drives and all(x.type is ObjT.drive for x in drives)
        assert all_unique(drives)

        pver_id = new_oid(ObjT.pver)  # "base" pool version
        m0conf[pver_id] = cls(
            data_units=pd_attrs.data_units,
            parity_units=pd_attrs.parity_units,
            spare_units=pd_attrs.spare_units,
            pool_width=len(drives),
            tolerance=list(tolerated_failures(m0conf, pd_attrs, drives)
                           if tolerance is None else tolerance),
            sitevs=[])
        m0conf[parent].pvers.append(pver_id)
        build_pver_subtree(m0conf, pver_id, drives)
        return pver_id


class ConfPverF(ToDhall):
    _cuid = _infinite_counter()  # cluster-unique pver_f identifier
    _objt = ObjT.pver_f
    _downlinks: Set[Downlink] = set()

    def __init__(self, cuid: int, base: Oid, allowance: List[int]):
        assert cuid >= 0
        assert base.type is ObjT.pver
        assert len(allowance) == len(pver_levels)
        all(n >= 0 for n in allowance)

        self.cuid = cuid
        self.base = base
        self.allowance = allowance

    def __repr__(self):
        args = ', '.join([f'cuid={self.cuid}',
                          f'base={self.base}',
                          f'allowance={self.allowance}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.pver_f
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'cuid = {self.cuid}',
                          f'base = {oid_to_dhall(self.base)}',
                          f'allowance = {self.allowance}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              base: Oid, allowance: Failures) -> Oid:
        assert parent.type is ObjT.pool

        pver_id = new_oid(ObjT.pver_f)
        m0conf[pver_id] = cls(cuid=next(cls._cuid),
                              base=base,
                              allowance=list(allowance))
        m0conf[parent].pvers.append(pver_id)
        return pver_id


def build_pver_subtree(m0conf: Dict[Oid, Any], pver: Oid,
                       drives: List[Oid]) -> None:
    assert pver.type is ObjT.pver
    assert drives and all(x.type is ObjT.drive for x in drives)

    # Maps real object to the corresponding virtual object.
    virtual: Dict[Oid, Oid] = {}

    for drive in drives:
        assert drive not in virtual
        assert pver not in m0conf[drive].pvers
        m0conf[drive].pvers.append(pver)
        virtual[drive] = ConfObjv.build(m0conf, real=drive)

        devs = [drive]
        parent_types = [getattr(ObjT, x) for x in pver_levels[:-1]]
        while parent_types:
            devs.insert(0, m0conf[devs[0]]._parent)
            dev = devs[0]
            assert dev.type is parent_types.pop()
            parent_ready = dev in virtual
            if not parent_ready:
                assert pver not in m0conf[dev].pvers
                m0conf[dev].pvers.append(pver)
                virtual[dev] = ConfObjv.build(m0conf, real=dev)
            m0conf[virtual[dev]].children.append(virtual[devs[1]])
            if parent_ready:
                break

        if not parent_types:
            m0conf[pver].sitevs.append(virtual[devs[0]])


class ConfObjv(ToDhall):
    _objt = ObjT.objv
    _downlinks = {Downlink('children', ObjT.objv)}

    def __init__(self, real: Oid, children: List[Oid]):
        assert real.type in [getattr(ObjT, x) for x in pver_levels]
        self.real = real
        assert all(x.type is ObjT.objv for x in children)
        self.children = children

    def __repr__(self):
        args = ', '.join([f'real={self.real}',
                          f'children={self.children}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.objv
        args = ', '.join([f'id={oid_to_dhall(oid)}',
                          f'real={oid_to_dhall(self.real)}',
                          f'children={oids_to_dhall(self.children)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], real: Oid) -> Oid:
        assert real in m0conf
        objv_id = new_oid(ObjT.objv)
        m0conf[objv_id] = cls(real, [])
        return objv_id


class ConfProfile(ToDhall):
    _objt = ObjT.profile
    _downlinks: Set[Downlink] = set()

    def __init__(self, pools: List[Oid], name: str):
        assert all(x.type is ObjT.pool for x in pools)
        self.pools = pools
        self._name = name

    def __repr__(self):
        args = ', '.join([f'pools={self.pools}',
                          f'name={self._name!r}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.profile
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'pools = {oids_to_dhall(self.pools)}'])
        return '{ %s }' % args

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              prof_desc: Dict[str, Any]) -> Oid:
        assert parent.type is ObjT.root

        pool_ids = dict((m0conf[pool_id]._name, pool_id) for pool_id in m0conf
                        if pool_id.type is ObjT.pool
                        and m0conf[pool_id]._name in prof_desc['pools'])
        prof_pools = [pool_ids[pool] for pool in prof_desc['pools']]

        root = m0conf[parent]
        dix_pools = [pool for pool in root.pools
                     if root.imeta_pver in m0conf[pool].pvers]
        assert len(dix_pools) == 1
        dix_pool = dix_pools[0]
        if dix_pool not in prof_pools:
            prof_pools.append(dix_pool)

        assert root.mdpool is not None
        if root.mdpool not in prof_pools:
            prof_pools.append(root.mdpool)

        prof_id = new_oid(ObjT.profile)
        m0conf[prof_id] = cls(pools=prof_pools, name=prof_desc['name'])
        m0conf[parent].profiles.append(prof_id)
        return prof_id


class ConfFdmiFltGrp(ToDhall):
    _objt = ObjT.fdmi_flt_grp
    _downlinks: Set[Downlink] = set()

    def __init__(self, rec_type: int, fdmi_filters: List[Oid]):
        assert all(x.type is ObjT.fdmi_filter for x in fdmi_filters)
        self.rec_type = rec_type
        self.fdmi_filters = fdmi_filters

    def __repr__(self):
        args = ', '.join([f'rec_type={self.rec_type}',
                          f'fdmi_filters={self.fdmi_filters!r}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.fdmi_flt_grp
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          f'rec_type = {self.rec_type}',
                          f'filters = {oids_to_dhall(self.fdmi_filters)}'])
        return f'{{ {args} }}'

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], parent: Oid,
              fdmi_filter_desc: Dict[str, Any],
              other_clients: Dict[str, List[Tuple[Oid, Oid]]]) -> Oid:
        assert parent.type is ObjT.root

        fdmi_flt_grp_id = new_oid(ObjT.fdmi_flt_grp)
        fdmi_filter_oid = ConfFdmiFilter.build(
            m0conf, parent, fdmi_flt_grp_id, fdmi_filter_desc, other_clients)
        # rec_type=0x1000 is M0_FDMI_REC_TYPE_FOL
        m0conf[fdmi_flt_grp_id] = cls(rec_type=0x1000,
                                      fdmi_filters=[fdmi_filter_oid])
        m0conf[parent].fdmi_flt_grps.append(fdmi_flt_grp_id)
        return fdmi_flt_grp_id


class ConfFdmiFilter(ToDhall):
    _objt = ObjT.fdmi_filter
    _downlinks: Set[Downlink] = set()

    def __init__(self, dix_pver: Oid, node: Oid,
                 substrings: List[str], endpoint: str):
        self.dix_pver = dix_pver
        self.node = node
        self.substrings = substrings
        self.endpoint = endpoint

    def __repr__(self):
        args = ', '.join([f'dix_pver={self.dix_pver}',
                          f'substrings={self.substrings}]',
                          f'endpoint={self.endpoint}'])
        return f'{self.__class__.__name__}({args})'

    def to_dhall(self, oid: Oid) -> str:
        assert oid.type is ObjT.fdmi_filter

        substrings = '[' + ', '.join([f'"{s}"' for s in self.substrings]) + ']'
        args = ', '.join([f'id = {oid_to_dhall(oid)}',
                          # 2 is M0_FDMI_FILTER_TYPE_KV_SUBSTRING
                          'filter_type = 2',
                          f'filter_id = {oid_to_dhall(oid)}',
                          'filter_root = ""',
                          f'node = {oid_to_dhall(self.node)}',
                          f'dix_pver = {oid_to_dhall(self.dix_pver)}',
                          f'substrings = {substrings} : List Text',
                          f'endpoints = ["{self.endpoint}"]'])
        return f'{{ {args} }}'

    @classmethod
    def build(cls, m0conf: Dict[Oid, Any], root: Oid, parent: Oid,
              fdmi_filter_desc: Dict[str, Any],
              other_clients: Dict[str, List[Tuple[Oid, Oid]]]) -> Oid:
        assert parent.type is ObjT.fdmi_flt_grp

        fdmi_filter_oid = new_oid(ObjT.fdmi_filter)
        node_oid, process_oid = other_clients[
            fdmi_filter_desc['node']][int(fdmi_filter_desc['client_index'])]
        process = m0conf[process_oid]
        m0conf[fdmi_filter_oid] = cls(
            dix_pver=m0conf[root].imeta_pver,
            node=node_oid,
            substrings=fdmi_filter_desc['substrings'],
            endpoint=str(process.endpoint))
        return fdmi_filter_oid


ConsulAgent = NamedTuple('ConsulAgent', [('node_name', str), ('ipaddr', str)])

Cluster = NamedTuple('Cluster', [('m0conf', Dict[Oid, Any]),
                                 ('consul_servers', List[ConsulAgent]),
                                 ('consul_clients', List[ConsulAgent]),
                                 ('m0_clients', Dict[Oid, Oid])])


def generate_consul_agents(cluster: Cluster) -> str:
    logging.debug('Generating consul configuration')
    assert cluster.consul_servers
    return json.dumps(
        {'servers': [x._asdict() for x in cluster.consul_servers],
         'clients': [x._asdict() for x in cluster.consul_clients]},
        indent=2) + '\n'


def consul_kv_drives(m0conf: Dict[Oid, Any]) -> Dict[str, str]:
    state_unknown = json.dumps({'state': 'M0_NC_UNKNOWN'})
    d = {}
    for site_id, site in m0conf.items():
        if site_id.type is not ObjT.site:
            continue
        site_key = 'm0conf/sites/' + oid_to_fidstr(site_id)
        d[site_key] = state_unknown
        for rack_id in site.racks:
            assert rack_id.type is ObjT.rack
            rack_key = f'{site_key}/racks/' + oid_to_fidstr(rack_id)
            d[rack_key] = state_unknown
            for encl_id in m0conf[rack_id].encls:
                assert encl_id.type is ObjT.enclosure
                encl_key = f'{rack_key}/encls/' + oid_to_fidstr(encl_id)
                d[encl_key] = json.dumps(
                    {'node': oid_to_fidstr(m0conf[encl_id].node),
                     'state': 'M0_NC_UNKNOWN'})
                for ctrl_id in m0conf[encl_id].ctrls:
                    assert ctrl_id.type is ObjT.controller
                    ctrl_key = f'{encl_key}/ctrls/' + \
                        oid_to_fidstr(ctrl_id)
                    d[ctrl_key] = state_unknown
                    for drive_id in m0conf[ctrl_id].drives:
                        assert drive_id.type is ObjT.drive
                        drive_key = f'{ctrl_key}/drives/' + \
                            oid_to_fidstr(drive_id)
                        d[drive_key] = json.dumps(
                            {'sdev': oid_to_fidstr(m0conf[drive_id].sdev),
                             'state': 'M0_NC_UNKNOWN'})
    return d


def consul_kv_sdevs(m0conf: Dict[Oid, Any]) -> Dict[str, str]:
    d = {}
    for node_id, node in m0conf.items():
        if node_id.type is not ObjT.node:
            continue
        node_key = 'm0conf/nodes/' + oid_to_fidstr(node_id)
        d[node_key] = json.dumps({'name': node.name,
                                  'state': 'M0_NC_UNKNOWN'})
        for proc_id in node.processes:
            assert proc_id.type is ObjT.process
            proc_key = f'{node_key}/processes/' + oid_to_fidstr(proc_id)
            # XXX-VERIFYME Can we really convert from portal (int) to name?
            proc_name = m0conf[proc_id].endpoint.portalgroup.name
            d[proc_key] = json.dumps({'name': proc_name,
                                      'state': 'M0_NC_UNKNOWN'})
            for svc_id in m0conf[proc_id].services:
                assert svc_id.type is ObjT.service
                svc_key = f'{proc_key}/services/' + oid_to_fidstr(svc_id)
                stype = m0conf[svc_id].type.name[len('M0_CST_'):].lower()
                d[svc_key] = json.dumps({'name': stype,
                                         'state': 'M0_NC_UNKNOWN'})
                for sdev_id in m0conf[svc_id].sdevs:
                    assert sdev_id.type is ObjT.sdev
                    sdev_key = f'{svc_key}/sdevs/' + oid_to_fidstr(sdev_id)
                    d[sdev_key] = json.dumps(
                        {'path': m0conf[sdev_id].filename,
                         'state': 'M0_NC_UNKNOWN'})
    return d


def generate_consul_kv(cluster: Cluster, dhall_dir: str) -> str:
    logging.debug('Generating consul KV structure')
    assert all(k.type is ObjT.process and v.type is ObjT.service
               for k, v in cluster.m0_clients.items())

    global fidk_gen
    _fidk_gen = next(fidk_gen)
    # Give up ownership of `fidk_gen`, passing it to the Consul KV.
    del fidk_gen

    m0conf = cluster.m0conf

    ConsulService = NamedTuple('ConsulService', [
        ('node_name', str), ('proc_id', Oid), ('ep', Any),
        ('svc_id', Oid), ('stype', str), ('meta_data', Optional[str])])

    def processes() -> Iterator[ConsulService]:
        for node_id, node in m0conf.items():
            if node_id.type is not ObjT.node:
                continue
            for proc_id in node.processes:
                assert proc_id.type is ObjT.process
                for svc_id in m0conf[proc_id].services:
                    stype = m0conf[svc_id].type.name[len('M0_CST_'):].lower()
                    proc = m0conf[proc_id]
                    yield ConsulService(node_name=node.name,
                                        proc_id=proc_id,
                                        ep=proc.endpoint,
                                        svc_id=svc_id,
                                        stype=stype,
                                        meta_data=proc.meta_data)
                # Add Motr clients to consul kv
                if proc_id in cluster.m0_clients:
                    proc_ep = m0conf[proc_id].endpoint
                    yield ConsulService(node_name=node.name,
                                        proc_id=proc_id,
                                        ep=proc.endpoint,
                                        svc_id=cluster.m0_clients[proc_id],
                                        stype=proc_ep.portalgroup.name,
                                        meta_data=None)

    def sns_pools() -> List[Oid]:
        for root_id, root in m0conf.items():
            if root_id.type is ObjT.root:
                return [x for x in root.pools
                        if x != root.mdpool and
                        root.imeta_pver not in m0conf[x].pvers]
        raise RuntimeError('Impossible happened')

    return json.dumps([dict(key=k, value=v) for k, v in (
        ('epoch', 1),
        ('eq-epoch', 1),
        ('leader', ''),
        ('last_fidk', _fidk_gen),
        ('failvec', ''),
        ('bytecount/healthy', 0),
        ('bytecount/degraded', 0),
        ('bytecount/critical', 0),
        ('bytecount/damaged', 0),
        *[(node.machine_id, node.name)
          for node_id, node in m0conf.items() if (node_id.type is ObjT.node
                                                  and node.machine_id)],
        *[('m0conf/pools/' + oid_to_fidstr(pool_id), m0conf[pool_id]._name)
          for pool_id in sns_pools()],
        *[('m0conf/profiles/' + oid_to_fidstr(prof_id),
           json.dumps({
               'name': prof._name,
               'pools': [m0conf[pool_id]._name for pool_id in prof.pools]
           }))
          for prof_id, prof in m0conf.items() if prof_id.type is ObjT.profile],
        *[(f'm0conf/nodes/{x.node_name}/processes/{x.proc_id.fidk}/services/'
           f'{x.stype}', x.svc_id.fidk) for x in processes()],
        *[(f'm0conf/nodes/{x.node_name}/processes/{x.proc_id.fidk}/'
           f'meta_data', x.meta_data)
          for x in processes() if x.stype == 'ios' and x.meta_data],
        *[(f'm0conf/nodes/{x.node_name}/processes/{x.proc_id.fidk}/endpoint',
           str(x.ep))
          for x in processes()],
        *consul_kv_sdevs(m0conf).items(),
        *consul_kv_drives(m0conf).items())],
        indent=2) + '\n'


def generate_confd(m0conf: Dict[Oid, ToDhall], dhall_dir: str) -> str:
    logging.debug('Generating confd.dhall')
    _assert(os.path.isabs(dhall_dir),
            f"Dhall directory doesn't exist: {dhall_dir}")

    def objt(obj: ToDhall) -> str:
        objt = obj.__class__.__name__
        assert objt.startswith('Conf')
        return objt[len('Conf'):]

    objs = '\n  , '.join('types.Obj.{} {}'.format(objt(v), v.to_dhall(k))
                         for k, v in m0conf.items())
    return f"""\
let types = {dhall_dir}/types.dhall
let utils = {dhall_dir}/utils.dhall

let objs =
  [ {objs}
  ]

in {dhall_dir}/toConfGen.dhall objs
"""


def validate_m0conf(m0conf: Dict[Oid, Any]) -> None:
    children_by_parent_type: Dict[ObjT, Dict[Downlink, List[Oid]]] = \
        dict((parent_t, {}) for parent_t in ObjT)
    for oid, obj in m0conf.items():
        assert oid.type is obj._objt
        assert obj.__class__.__name__ == \
            'Conf' + ''.join(s.capitalize() for s in oid.type.name.split('_'))
        for rel in obj._downlinks:
            children = getattr(obj, rel.name)
            assert all_unique(children)
            assert all(
                (x.type is rel.children_t or
                 # XXX leaky abstraction
                 (x.type is ObjT.pver_f and rel.children_t is ObjT.pver))
                for x in children)
            children_by_parent_type[oid.type].setdefault(rel.name, []) \
                                             .extend(children)
    assert all(all_unique(children)
               for _parent_t, rel_children in children_by_parent_type.items()
               for _rel, children in rel_children.items())


# This function will return list of disks present in provided pool
# It will return None if 'disk_refs' is not present in pool
def get_pool_diskrefs(pool_desc: Dict[str, Any]):
    return pool_desc.get('disk_refs')


# AUX Pools :
# This feature is developed considering the node (enclosure) failure
# case. If a node in the cluster fails, it means that all the disks
# in that node become inaccessible. As this feature considers only
# node failures, it keeps the spare pools ready considering the
# other alive nodes from the cluster.
# The feature works using the mathematical combinations formula
# C(n, r) = (n! / (r!(n-r)!)).
# It basically creates combinations of all the nodes in case of
# failures of nodes from 1 to the allowed number of nodes to be
# failed, by allowance vector.
# For each and every such generated combinations of (alive) nodes,
# it creates the sets of the disks and assigns a pool to each of
# them. Such spare pools created considering allowed node failures
# are called as aux pools.

# Function : aux_pool_list() :
# Description : Generate the aux pools sets
# Inputs :
# pool_desc : Pools in the cluster description
# m0conf : Configuration
# Outputs : None
# Saves generated aux pools list in aux_pools[]

def aux_pool_list(pool_desc: Dict[str, Any],
                  disks: List[DiskRef],
                  m0conf: Dict[Oid, Any],
                  cluster_desc: Dict[str, Any]) -> None:
    pool = pool_desc

    # Example of encl_ref_combination :
    # [
    #   ['ssc-vm-6327.colo.seagate.com', 'ssc-vm-2493.colo.seagate.com'],
    #   ['ssc-vm-6327.colo.seagate.com', 'ssc-vm-6328.colo.seagate.com'],
    #   ['ssc-vm-2493.colo.seagate.com', 'ssc-vm-6328.colo.seagate.com']
    # ]
    encl_ref_combination: List[List[str]] = []

    # Example of disk_ref_combinations :
    # [
    #   [
    #     {'path': '/dev/sdb', 'node': 'ssc-vm-6327.colo.seagate.com'},
    #     {'path': '/dev/sdc', 'node': 'ssc-vm-6327.colo.seagate.com'},
    #     {'path': '/dev/sdb', 'node': 'ssc-vm-2493.colo.seagate.com'},
    #     {'path': '/dev/sdc', 'node': 'ssc-vm-2493.colo.seagate.com'}
    #   ],
    #   [ ... Similarly for other sets.
    #   ]
    # ]
    disk_ref_combinations: List[List[Dict[str, Any]]] = []

    # Example of pool_nodes :
    # [
    #  'ssc-vm-6327.colo.seagate.com',
    #  'ssc-vm-2493.colo.seagate.com',
    #  'ssc-vm-6328.colo.seagate.com'
    # ]
    pool_nodes: List[str] = []

    tolerance: Failures
    pool_t = pool_type(pool)

    # Currently assumption is that, aux pools will be generated for
    # SNS pools only
    _assert(
        pool_t == PoolT.sns,
        f"Pool {pool['name']}: aux pools are supported for SNS pools only")

    # Calculate and create aux pool disk combinations
    # only for SNS pool type
    d = pool.get('allowed_failures')
    # Calculate the tolerance in case of SNS pools
    if d:
        tolerance = Failures(d['site'], d['rack'], d['encl'],
                             d['ctrl'], d['disk'])
    else:
        tolerance = ConfPool.gen_sns_tolerance(m0conf, pool, cluster_desc)
    # Generate the node combinations considering node failures as
    # allowed by node tolerance values
    temp_pool_nodes = []

    disk_refs = get_pool_diskrefs(pool)
    if disk_refs is None:
        for disk in disks:
            temp_pool_nodes += [
                disk.node]
    else:
        for nodes in disk_refs:
            temp_pool_nodes += [
                nodes['node']]
    pool_nodes = list(set(temp_pool_nodes))
    for encl_failures in range(tolerance.enclosure, 0, -1):
        encl_ref_combination += [
            list(x) for x in itertools.combinations(
                pool_nodes,
                (len(pool_nodes) - encl_failures))
        ]

    # Create the disk references for the generated
    # node combinations
    for encl in encl_ref_combination:
        single_aux_comb: List[Dict[str, Any]] = []
        for encl_x in encl:
            if disk_refs is None:
                for disk in disks:
                    if disk.node == encl_x:
                        data = {}
                        data['node'] = disk.node
                        data['path'] = disk.path
                        single_aux_comb += [data]
            else:
                for dr in disk_refs:
                    if dr['node'] == encl_x:
                        single_aux_comb += [dr]
        disk_ref_combinations += [single_aux_comb]
    # Create list of auxilary pools
    aux_pools[pool['name']] = [
        {'name':         pool['name'] + f'-aux{i:003}',
         'data_units':   pool['data_units'] - pool['parity_units'],
         'parity_units': pool['parity_units'],
         'spare_units':  pool['spare_units'],
         'disk_refs':    comb}
        for i, comb in enumerate(disk_ref_combinations, start=1)
    ]


def build_cluster(cluster_desc: Dict[str, Any]) -> Cluster:
    cluster = Cluster(m0conf={}, consul_servers=[], consul_clients=[],
                      m0_clients={})
    conf = cluster.m0conf
    root_id = ConfRoot.build(conf)
    # XXX Move all the logic into ConfRoot.build?

    rack_id = ConfRack.build(conf, ConfSite.build(conf, root_id))

    other_clients: Dict[str, List[Tuple[Oid, Oid]]] = {}
    io_ctrls: Dict[Oid, Any] = {}

    ports = cluster_desc.get('network_ports')
    if ports:
        lnet_portals = get_lnet_portal_group(
                           hax=ports['hax'],
                           m0_server=ports['m0_server'],
                           m0_client_s3=ports['m0_client_s3'],
                           m0_client_other=ports['m0_client_other'])
        libfab_portals = get_libfab_portal_group(
                             hax=ports['hax'],
                             m0_server=ports['m0_server'],
                             m0_client_s3=ports['m0_client_s3'],
                             m0_client_other=ports['m0_client_other'])
    else:
        lnet_portals = get_lnet_portal_group()
        libfab_portals = get_libfab_portal_group()

    for node in cluster_desc['nodes']:
        node_id = ConfNode.build(conf, root_id, node)
        encl_id = ConfEnclosure.build(conf, rack_id, node_id)
        ctrl_ids: List[Tuple[Optional[Oid], Any]] = []
        m0srv = node.get('m0_servers')
        if m0srv is not None:
            for m0d in node.get('m0_servers', []):
                if m0d['_io_disks']:
                    ctrl_oid = ConfController.build(conf, encl_id)
                    ctrl_ids.append((ctrl_oid, m0d))
                    io_ctrls[ctrl_oid] = m0d
                else:
                    ctrl_ids.append((None, m0d))
        transport_type = node.get('transport_type', 'libfab')
        if transport_type == 'lnet':
            portalgroup = lnet_portals
        else:
            portalgroup = libfab_portals

        ConfProcess.build(conf, node_id, portalgroup, node, ProcT.hax)

        m0srv = node.get('m0_servers')
        confd_p = False
        if m0srv is not None:
            for ctrl_id in ctrl_ids:
                m0d = ctrl_id[1]
                conf_ctrl_id: Optional[Oid] = ctrl_id[0]
                ConfProcess.build(conf, node_id, portalgroup, node,
                                  ProcT.m0_server, m0d, conf_ctrl_id)
                if m0d['runs_confd']:
                    confd_p = True

        ipaddr = node['facts'][ipaddr_key(node['data_iface'])]
        (cluster.consul_servers if confd_p else cluster.consul_clients).\
            append(ConsulAgent(node_name=node['hostname'],
                               # XXX use 'mgmt_iface' for consul?
                               ipaddr=ipaddr))

        for client_t, proc_t in [('s3', ProcT.m0_client_s3),
                                 ('other', ProcT.m0_client_other)]:
            for _ in range(node['m0_clients'][client_t]):
                proc_id = ConfProcess.build(conf, node_id, portalgroup, node,
                                            proc_t)
                cluster.m0_clients[proc_id] = new_oid(ObjT.service)
                if client_t == 'other':
                    if node['hostname'] not in other_clients:
                        other_clients[node['hostname']] = []
                    other_clients[node['hostname']].append((node_id, proc_id))

    # Motr requires all the data devices together in-order to map a data object
    # to its corresponding device and further to its ioservice. Motr maintains
    # a global pool wide device to service context array which is expected to
    # be long as the number of data devices in the pool. Adding CAS device in
    # between the data devices voids this assumption in motr code. Thus we
    # create metadata devices after creating all the data devices in the pool.
    build_cas_devs(conf, io_ctrls)
    check_drive_multiple_references(conf, cluster_desc)

    create_aux_val = cluster_desc.get('create_aux')
    if not create_aux_val:
        for pool in cluster_desc['pools']:
            ConfPool.build(conf, root_id, pool, cluster_desc)
    else:
        for pool in cluster_desc['pools']:
            pool_t = pool_type(pool)
            if pool_t == PoolT.sns:
                # For SNS pools create the aux pool list
                disks: List[DiskRef] = []
                # We need to generate disk_refs only if it is not present
                # in pool description
                if not get_pool_diskrefs(pool):
                    for node in cluster_desc['nodes']:
                        if not node.get('m0_servers'):
                            continue
                        for m0d in node['m0_servers']:
                            if m0d['runs_confd']:
                                continue
                            for d in m0d['io_disks']['data']:
                                disks.append(DiskRef(path=d,
                                                     node=node['hostname']))

                aux_pool_list(pool, disks, conf, cluster_desc)
            else:
                ConfPool.build(conf, root_id, pool, cluster_desc)

        for pool in itertools.chain(cluster_desc['pools'],
                                    *aux_pools.values()):
            pool_t = pool_type(pool)
            if pool_t == PoolT.sns:
                ConfPool.build(conf, root_id, pool, cluster_desc)

    if conf[root_id].imeta_pver is None:  # no DIX pool defined in the CDF
        ConfPool.build(conf, root_id, {'type': 'dix'}, cluster_desc)

    if conf[root_id].mdpool is None:  # no Metadata pool defined in the CDF
        ConfPool.build(conf, root_id, {'type': 'md'}, cluster_desc)

    conf[root_id].mdredundancy = len(cluster_encls(conf, SvcT.M0_CST_IOS))

    for prof in cluster_desc['profiles']:
        if not create_aux_val:
            ConfProfile.build(conf, root_id, prof)
        else:
            prof['pools'] += [aux_pool['name']
                              for pool_name in prof['pools']
                              for aux_pool in aux_pools.get(pool_name, [])]
            ConfProfile.build(conf, root_id, prof)

    if 'fdmi_filters' in cluster_desc:
        if cluster_desc['fdmi_filters'] is not None:
            for fdmi_filter in cluster_desc['fdmi_filters']:
                ConfFdmiFltGrp.build(conf, root_id, fdmi_filter, other_clients)

    validate_m0conf(conf)
    _assert(
        bool(cluster.consul_servers), 'No consul agent is required by the '
        'current cluster description which is logically incorrect')
    names = [x.node_name
             for x in cluster.consul_servers + cluster.consul_clients]
    _assert(all_unique(names), f'Consul agent names are not unique: {names}')
    _assert(all(re.search('[",= @]', name) is None for name in names),
            'Some consul agent names use unsupported symbols '
            f'(check for ",= @): {names}')
    _assert(
        all_unique(x.ipaddr
                   for x in cluster.consul_servers + cluster.consul_clients),
        'Some of the consul agent IP addresses are not unique')
    return cluster


if __name__ == '__main__':
    main()
