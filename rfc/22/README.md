---
domain: github.com
shortname: 22/MPAUX
name: Multipool Auxiliary Layout Parameters Generation
status: raw
editors: Suvrat Joshi <suvrat.joshi@seagate.com>
---

## Language

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in [RFC 2119](https://tools.ietf.org/html/rfc2119).

## Abstract

The multipools feature gives cluster an ability to have multiple pools (sets) of the disk/storage resources and IO's can be done on the desired pool.
The auxiliary pools feature is required for creating auxialiry pool sets, one of which could be used upon failure of one or more disk resources.

## Design

If some disk resources in a cluster fail for some reason temporarily (because of reasons like reboot or transient network error etc.), then it is expected that cluster still works properly with reduced pool layout for failure handling. Let's say the initial pool layout configuration is 4+2, and a storage set has 6 data volumes. If 1-2 of those data volumes fail, the pool is switched to 2+2 layout on the remaining data volumes. There can be several combinations of the 2+2 data volume layouts, depending on which particular two volumes fail. All such combinations need to be pre-populated (auto-generated by Hare from the CDF configuration) by the hctl bootstrap script.

### Details with example
Each storage set corresponds to a standalone Motr pool. For the sake of practicality, this description will assume a default storage set configuration, which consists of six nodes, srvnode-1..6 (three enclosures, each has two controllers). Where every storage set pool has 4+2 layout, which means â€” four data units(N) and two parity units(K) in a parity group.
Typical configuration of such pool in the CDF format, expected by [`cfgen`](https://github.com/Seagate/cortx-hare/blob/dev/rfc/18/README.md), would be something like this:
```yaml
pools:
  - name: storage-set01
    disk_refs:
      - { path: /dev/mpath1, node: srvnode-1 }
      - { path: /dev/mpath2, node: srvnode-2 }
      - { path: /dev/mpath3, node: srvnode-3 }
      - { path: /dev/mpath4, node: srvnode-4 }
      - { path: /dev/mpath5, node: srvnode-5 }
      - { path: /dev/mpath6, node: srvnode-6 }
    data_units: 4
    parity_units: 2
    allowed_failures: { site: 0, rack: 0, encl: 0, ctrl: 0, disk: 2 }
```
Such a pool can tolerate a failure of two distinct mpath storage devices, out of six total devices. For performance considerations, when there are only four storage devices remain in the pool, due to a failure of the other two mpath, it's not efficient to write data using original 4+2 layout. It's better to use 2+2 layout. For this reason, additional pool configurations needs to be pre-generated by cfgen for every original pool configuration in the CDF. Every such configuration will have -auxNN suffix added (stands for auxiliary) to the original pool name, where NN is a sequential pool configuration number. For example, for a case where srvnode-5 and srvnode-6 have failed, an auxiliary pool config, auto-generated internally by cfgen would look like this:
```yaml
pools:
  - name: storage-set01
    disk_refs:
      - { path: /dev/mpath1, node: srvnode-1 }
      - { path: /dev/mpath2, node: srvnode-2 }
      - { path: /dev/mpath3, node: srvnode-3 }
      - { path: /dev/mpath4, node: srvnode-4 }
    data_units: 2
    parity_units: 2
    allowed_failures: { site: 0, rack: 0, encl: 0, ctrl: 0, disk: 2 }
```
We have to calculate all possible combinations (without repetition) of the four surviving srvnode's out of the total six nodes, and cfgen needs to generate pool configuration with reduced 2+2 layout for each of them internally. There will 15 of them, according to the formula: C(p,k) = p! / ( k! * (p - k)! ). In our case p := 6 and k := 4:
```
srvnode1	srvnode1	srvnode1
srvnode2	srvnode2	srvnode2
srvnode3	srvnode3	srvnode3
srvnode4	srvnode5	srvnode6

srvnode1	srvnode1	srvnode1
srvnode2	srvnode2	srvnode2
srvnode4	srvnode4	srvnode5
srvnode5	srvnode6	srvnode6

srvnode1	srvnode1	srvnode1
srvnode3	srvnode3	srvnode3
srvnode4	srvnode4	srvnode5
srvnode5	srvnode6	srvnode6

srvnode1	srvnode2	srvnode2
srvnode4	srvnode3	srvnode3
srvnode5	srvnode4	srvnode4
srvnode6	srvnode5	srvnode6

srvnode2	srvnode2	srvnode3
srvnode3	srvnode4	srvnode4
srvnode5	srvnode5	srvnode5
srvnode6	srvnode6	srvnode6
```
The resulting confd.dhall output of a new cfgen implementation should be identical to the output generated by a current cfgen, as if the following CFD was provided as its input:
```
pools:
  - name: storage-set01
    disk_refs:
      - { path: /dev/mpath1, node: srvnode-1 }
      - { path: /dev/mpath2, node: srvnode-2 }
      - { path: /dev/mpath3, node: srvnode-3 }
      - { path: /dev/mpath4, node: srvnode-4 }
      - { path: /dev/mpath5, node: srvnode-5 }
      - { path: /dev/mpath6, node: srvnode-6 }
    data_units: 4
    parity_units: 2

  - name: storage-set01-aux01
    disk_refs:
      - { path: /dev/mpath1, node: srvnode-1 }
      - { path: /dev/mpath2, node: srvnode-2 }
      - { path: /dev/mpath3, node: srvnode-3 }
      - { path: /dev/mpath4, node: srvnode-4 }
    data_units: 2
    parity_units: 2

  - name: storage-set01-aux02
    disk_refs:
      - { path: /dev/mpath1, node: srvnode-1 }
      - { path: /dev/mpath2, node: srvnode-2 }
      - { path: /dev/mpath3, node: srvnode-3 }
      - { path: /dev/mpath5, node: srvnode-5 }
    data_units: 2
    parity_units: 2

  # ... skiped other 12 aux configs ...

  - name: storage-set01-aux15
    disk_refs:
      - { path: /dev/mpath3, node: srvnode-3 }
      - { path: /dev/mpath4, node: srvnode-4 }
      - { path: /dev/mpath5, node: srvnode-5 }
      - { path: /dev/mpath6, node: srvnode-6 }
    data_units: 2
    parity_units: 2
```

Above scheme discusses only reduced configuration of 2+2, but we can similarly have reduced 3+2 configuration in case of single device failure.

When multipool configuration with auxiliary pools is bootstrapped, `hctl status` command should have pools/profile information similar to shown as below:
```
Data pools:
    # fid name
    0x6f00000000000001:0x77 '# hctl status
Data pools:
    # fid name
    0x6f00000000000001:0x77 'storage-set01'
    0x6f00000000000001:0x8b 'storage-set01-aux01'
    0x6f00000000000001:0x9b 'storage-set01-aux02'
    0x6f00000000000001:0xab 'storage-set01-aux03'
    0x6f00000000000001:0xb9 'storage-set01-aux04'
    # ... skiped other 12 aux configs ...
    0x6f00000000000001:0xc7 'storage-set01-aux15'
Profile:
    # fid name: pool(s)
    0x7000000000000001:0xfd 'pool': 'storage-set01' 'storage-set01-aux01' 'storage-set01-aux02' 'storage-set01-aux03' 'storage-set01-aux04' ... 'storage-set01-aux15' None None
Services:
    # ... skipping other unchanged status part
```

