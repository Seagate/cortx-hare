#!/usr/bin/env bash
#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# For any questions about this software or licensing,
# please email opensource@seagate.com or cortx-questions@seagate.com.

set -eu -o pipefail
# set -x
export PS4='+ [${BASH_SOURCE[0]##*/}:${LINENO}${FUNCNAME[0]:+:${FUNCNAME[0]}}] '

# :help: bootstrap the cluster

PROG=${0##*/}

usage() {
    cat <<EOF
Usage: $PROG [<option>]... <CDF>
       $PROG [<option>]... --conf-dir <dir>

Bootstrap the cluster.

Positional arguments:
  <CDF>                  Path to the cluster description file.
  -c, --conf-dir <dir>   Don't generate configuration files, use existing
                         ones from the specified directory.

Options:
  --debug      Print commands and their arguments as they are executed.
  --mkfs       Execute m0mkfs.  *CAUTION* This wipes all Motr data!
  -h, --help   Show this help and exit.
EOF
}

die() {
    echo "$PROG: $*" >&2
    exit 1
}

say() {
    echo -n "$(date '+%F %T'): $*"
}

ssh_error_info() {
    local hostname=$1
    shift
    ssh -n $hostname "$@" ||
    {
        echo "Error at $hostname with command $@";
        exit 1
    }
}

conf_dir_perm_check() {
    ssh -n $node """
    if ! [[ -d $conf_dir ]]; then
        echo "$conf_dir directory does not exist on $node \
Try reinstalling Hare on the above mentioned node.";
        exit 1
    fi

    if ! [[ -w $conf_dir ]]; then
        echo "Cannot write to $conf_dir directory of $node";
        exit 1
    fi
    echo $node > $conf_dir/node-name;
"""
}

get_server_nodes() {
    # Produces the output like this:
    #
    # localhost 10.230.240.139
    # localhost2 10.230.240.140
    #
    jq -r '.servers[] | "\(.node_name) \(.ipaddr)"' \
       $conf_dir/consul-agents.json
}

get_client_nodes() {
    jq -r '.clients[] | "\(.node_name) \(.ipaddr)"' \
       $conf_dir/consul-agents.json
}

get_all_nodes() {
    jq -r '(.servers + .clients)[] | "\(.node_name) \(.ipaddr)"' \
        $conf_dir/consul-agents.json
}

get_session() {
    consul kv get -detailed leader | awk '/Session/ {print $2}'
}

get_session_checks_nr() {
    local sid=$1
    curl -sX GET http://localhost:8500/v1/session/info/$sid |
        jq -r '.[].Checks|length'
}

wait_rc_leader() {
    local count=1
    while [[ $(get_session) == '-' ]]; do
        if (( $count > 5 )); then
            consul kv put leader elect$RANDOM > /dev/null
            count=1
        fi
        sleep 1
        echo -n '.'
        (( count++ ))
    done
}

wait4() {
    for pid in $*; do
        wait $pid
    done
}

get_ready_agents() {
    consul members | sed 1d | awk '{print $1}'
}

get_ready_agents_nr() {
    consul members | sed 1d | wc -l
}

abort_if_RC_leader_election_is_impossible() {
    local ssh node confd_id svc cmd
    local status_commands=()

    # This code assumes that there is at most one confd server per node.
    # This assumption is enforced by `cfgen`.
    while IFS=/ read node confd_id; do
        local ok=true
        confd_id="m0d@$(printf 0x7200000000000001:0x%x $confd_id)"
        if [[ $node == $(node-name) ]]; then
            ssh=
        else
            ssh="ssh_error_info $node"
        fi

        for svc in hare-hax $confd_id; do
            if $ssh sudo systemctl --quiet --state=failed is-failed $svc; then
                # hare-hax service fails during stop which blocks the further
                # deployment of the cluster. Remove this explicit reset-failed
                # and enable code after this statement once the stop failure is
                # resolved.
                sudo systemctl reset-failed $svc

                #ok=false
                #status_commands+=(
                #    "${ssh:+$ssh }sudo systemctl status $svc"
                #)
            fi
        done
        if $ok; then
            # RC Leader session can be created on at least one Consul server
            # node.
            return 0
        fi
    done < <(jq -r '.[] | .key' $conf_dir/consul-kv.json |
                grep '/services/confd$' |
                # m0conf/nodes/<name>/processes/<process_id>/services/confd
                cut -d/ -f3,5
            )
    cat >&2 <<EOF
**ERROR** RC leader election is guaranteed to fail.

The RC leader can only be elected if there is a Consul server node
on which both hare-hax and confd m0d@ service are in "non-failed" state.

To see details, type
$(for cmd in "${status_commands[@]}"; do
    echo "    $cmd"
done)

To reset the "failed" state, type
$(for cmd in "${status_commands[@]}"; do
    echo "    ${cmd/ status/ reset-failed}"
done)
EOF
    exit 1
}

is_localhost() {
    (($# == 1)) && [[ -n $1 ]] || die "${FUNCNAME[0]}: Invalid usage"
    local node=$1
    case $node in
        localhost|127.0.0.1|$(hostname)|$(hostname --fqdn)) return 0;;
    esac
    local path=/etc/salt/minion_id
    [[ -e $path && $(cat $path) == $node ]] && return 0 || return 1
}

check_consul_failure() {
    while read node bind_ip; do
        if ssh -n $node "sudo systemctl --quiet --state=failed is-failed hare-consul-agent"; then
            echo 'Failed to start agent on '$node: >&2
            true; return
        fi
    done < <(get_all_nodes)

    false; return
}

# --------------------------------------------------------------------
# main

TEMP=$(getopt --options hc: \
              --longoptions help,mkfs,conf-dir:,debug \
              --name "$PROG" -- "$@" || true)

(($? == 0)) || { usage >&2; exit 1; }

eval set -- "$TEMP"

conf_dir=
opt_mkfs=
debug_p=false

while true; do
    case "$1" in
        -h|--help)           usage; exit ;;
        --mkfs)              opt_mkfs=--mkfs; shift ;;
        -c|--conf-dir)       conf_dir=$2; shift 2 ;;
        --debug)             debug_p=true; shift ;;
        --)                  shift; break ;;
        *)                   break ;;
    esac
done

case $# in
    0) [[ -d $conf_dir ]] || die "'--conf-dir' argument is not a directory";;
    1) [[ -z $conf_dir ]] || { usage >&2; exit 1; };;
    *) die 'Too many arguments';;  # unreachable (ruled out by getopt)
esac

$debug_p && set -x

cdf=${1:-}

if sudo systemctl --quiet is-active hare-hax; then
    die 'hare-hax is active ==> cluster is already running'
fi

if [[ -z $conf_dir ]]; then
    conf_dir=/var/lib/hare

    if ! [[ -d $conf_dir ]]; then
        cat <<EOF >&2
$conf_dir directory does not exist.
Try reinstalling Hare.
EOF
        exit 1
    fi
    if ! [[ -w $conf_dir ]]; then
        cat <<EOF >&2
Cannot write to $conf_dir directory.

Did you forget to add current user ($USER) to 'hare' group?
If so, run
    sudo usermod --append --groups hare $USER
then re-login and try to bootstrap again.
EOF
        exit 1
    fi

    say 'Generating cluster configuration...'
    PATH="$(dirname $(readlink -f $0)):$PATH" cfgen -o $conf_dir $cdf
    dhall text < $conf_dir/confd.dhall | m0confgen > $conf_dir/confd.xc

    while read node _; do
        if is_localhost $node; then
            echo $node > $conf_dir/node-name
        else
            # Redirect stdin to /dev/null (`-n`) to prevent
            # accidental stealing of `get_all_nodes` output.
            conf_dir_perm_check
        fi
    done < <(get_all_nodes)

    echo ' OK'
fi

abort_if_RC_leader_election_is_impossible

read _ join_ip <<< $(get_server_nodes | grep -w $(node-name))

# get_server_nodes will produce multiple lines. We'll join them with space.
# Note: in case when get_server_nodes is empty list, consequent awk and tr
# will exit with non-zero code.
# In order to swallow this error, `|| true` is needed.

join_peers_opt=$(get_server_nodes | grep -vw "$(node-name)" | awk '{print "--join " $2}' | \
              tr '\n' ' ' || true)

if [[ -z $join_ip ]]; then
    cat <<'EOF' >&2
Bootstrap should be executed on a Consul server node.

"Consul server nodes" are the nodes with 'runs_confd: true' in the CDF
(cluster description file).
EOF
    exit 1
fi

if sudo systemctl --quiet is-active hare-consul-agent; then
    say 'Stopping Consul on this node...'
    sudo systemctl stop hare-consul-agent
    echo ' OK'

    say 'Stopping Consul on other nodes...'
    pids=()
    while read node bind_ip; do
        ssh_error_info "$node" "sudo systemctl stop hare-consul-agent" &
        pids+=($!)
    done < <(get_server_nodes | grep -vw $(node-name) || true)

    while read node bind_ip; do
        ssh_error_info "$node" "sudo systemctl stop hare-consul-agent" &
        pids+=($!)
    done < <(get_client_nodes)
    wait4 ${pids[@]-}
    echo ' OK'
fi

say 'Starting Consul server on this node...'

# $join_ip is our bind_ip address
mk-consul-env --mode server --bind $join_ip $join_peers_opt \
              --extra-options '-ui -bootstrap-expect 1'

sudo systemctl start hare-consul-agent

# Wait for Consul's internal leader to be ready.
# (Until then the KV store won't be accessible.)
while ! consul info 2>/dev/null | grep -q 'leader.*true'; do
    sleep 1
    echo -n '.'
done
echo ' OK'

say 'Importing configuration into the KV store...'
jq '[.[] | {key, value: (.value | @base64)}]' < $conf_dir/consul-kv.json |
    consul kv import - > /dev/null
echo ' OK'

say 'Starting Consul on other nodes...'
pids=()
while read node bind_ip; do
    ssh_error_info "$node" "PATH=$PATH $(which mk-consul-env) \
                          --mode server --bind $bind_ip --join $join_ip &&
               sudo systemctl start hare-consul-agent" &
    pids+=($!)
done < <(get_server_nodes | grep -vw $(node-name) || true)

while read node bind_ip; do
    ssh_error_info "$node" "PATH=$PATH $(which mk-consul-env) \
                          --mode client --bind $bind_ip --join $join_ip &&
               sudo systemctl start hare-consul-agent" &
    pids+=($!)
done < <(get_client_nodes)
wait4 ${pids[@]-}
agents_nr=$(( ${#pids[@]} + 1 ))
count=1

# Waiting for the Consul agents to get ready...
while (( $(get_ready_agents_nr) != $agents_nr )); do
    if (( $count > 5 )); then
        echo 'Consul not ready on node :'
        comm -13 <(get_ready_agents | sort) \
                 <(get_all_nodes | awk '{print $1}' | sort) >&1
        count=1

        if check_consul_failure; then
            echo 'Some agent(s) failed to start:' >&2
            comm -13 <(get_ready_agents | sort) \
                     <(get_all_nodes | awk '{print $1}' | sort) >&2
            echo 'Check connectivity and firewall (Consul ports must be opened)' >&2
            exit 1
        fi
    fi
    (( count++ ))
    
    sleep 1
done
echo 'Consul ready on all nodes'

say 'Updating Consul configuraton from the KV store...'
update-consul-conf &
pids=($!)
while read node _; do
    ssh_error_info "$node" "PATH=$PATH $(which update-consul-conf)" &
    pids+=($!)
done < <(get_all_nodes | grep -vw $(node-name) || true)
wait4 ${pids[@]}
echo ' OK'

# Installing Motr confd.xc files...
while read node _; do
    scp -q $conf_dir/confd.xc $node:$conf_dir
done < <(get_server_nodes | grep -vw $(node-name) || true)

say 'Waiting for the RC Leader to get elected...'
wait_rc_leader
sid=$(get_session)
# There is always the serfHealth check in the session. But
# if it is the only one - we should destroy the current session
# (and wait for re-election to happen) to make sure that the new
# session will be bound to the Motr services checks also.
while (( $(get_session_checks_nr $sid) == 1 )); do
    curl -sX PUT http://localhost:8500/v1/session/destroy/$sid &>/dev/null
    wait_rc_leader
    sid=$(get_session)
done
echo ' OK'

get_nodes() {
    local phase=$1

    if [[ $phase == phase1 ]]; then
        # Note: confd-s are running on server nodes only.
        get_server_nodes
    else
        get_all_nodes
    fi
}

start_motr() {
    local op=$1
    local phase=$2

    say "Starting Motr ($phase, $op)..."
    [[ $op == 'mkfs' ]] && op='--mkfs-only' || op=
    bootstrap-node $op --phase $phase &
    pids=($!)

    while read node _; do
        ssh_error_info "$node" "PATH=$PATH $(which bootstrap-node) $op --phase $phase" &
        pids+=($!)
    done < <(get_nodes $phase | grep -vw $(node-name) || true)
    wait4 ${pids[@]}
    echo ' OK'
}

# Start Motr in two phases: 1st confd-s, then ios-es.
bootstrap_nodes() {
    local phase=$1

    if [[ $opt_mkfs ]]; then
        start_motr 'mkfs' $phase
    fi
    start_motr 'm0d' $phase
}

# Start confds first
bootstrap_nodes phase1

# Start ioservices
bootstrap_nodes phase2

. update-consul-conf --dry-run  # import S3_IDs
if [[ -n $S3_IDs ]]; then
    # Now the 3rd phase (s3servers).
    say 'Starting S3 servers (phase3)...'
    bootstrap-node --phase phase3 &
    pids=($!)

    while read node _; do
        ssh_error_info "$node" "PATH=$PATH $(which bootstrap-node) --phase phase3" &
        pids+=($!)
    done < <(get_all_nodes | grep -vw $(node-name) || true)
    wait4 ${pids[@]}
    echo ' OK'
fi

say 'Checking health of services...'
check_service() {
    local svc=$1
    curl -s http://127.0.0.1:8500/v1/health/service/$svc |
        jq -r '.[] | "\(.Node.Node) \([.Checks[].Status]|unique)"' |
        fgrep -v '["passing"]' || true
}
count=1
for svc in confd ios s3service; do
    svc_not_ready=$(check_service $svc)
    while [[ $svc_not_ready ]]; do
        if (( $count > 30 )); then
            echo $svc_not_ready >&2
            echo "Check '$svc' service on the node(s) listed above." >&2
            exit 1
        fi
        (( count++ ))
        sleep 1
        svc_not_ready=$(check_service $svc)
    done
done
echo ' OK'
